\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Keep aspect ratio if custom image width or height is specified
    \setkeys{Gin}{keepaspectratio}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{soul}      % strikethrough (\st) support for pandoc >= 3.0.0
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{project\_code}
    
    
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@ges}{\let\PY@bf=\textbf\let\PY@it=\textit}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \section{Exploration, Grouping, and Prediction of Genre in Spotify
Songs}\label{exploration-grouping-and-prediction-of-genre-in-spotify-songs}

\subsection{Team Spotiflies: Joanna, Aaron, Aubrey, Kennedy, Aster,
Ethan}\label{team-spotiflies-joanna-aaron-aubrey-kennedy-aster-ethan}

GitHub Link: https://github.com/ketexon/csm148-spotiflies

    The data set that we chose for the project was the Spotify dataset,
which is a dataset with information about the popularity, genre, and
several other musical qualities of over 140K songs on the Spotify music
streaming platform. The link to the original dataset can be found here:
https://huggingface.co/datasets/maharshipandya/spotify-tracks-dataset.

The main feature we're interested in with this dataset is song genre,
and how it relates to other features. We want to see if the musical
qualities of a song are strong influences of what genre it's
categorized. The dataset has 114 different genres, ranging from emo to
classical to industrial and more. We are interested in answering
questions regarding what, if anything, in songs may be a predictor of
what genre it is, if certain genres are more similar to each other than
others, and more.

    \subsection{First Looks and Cleaning Our
Data}\label{first-looks-and-cleaning-our-data}

We started our project by investigating our data for any obvious
cleaning we have to do before EDA, such as missing data or unreasonable
values.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{50}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}}\PY{k}{pip} install pandas numpy matplotlib seaborn scikit\PYZhy{}learn mlxtend
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages
(2.2.3)
Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages
(2.1.3)
Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-
packages (3.9.3)
Requirement already satisfied: seaborn in ./.venv/lib/python3.12/site-packages
(0.13.2)
Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-
packages (1.5.2)
Requirement already satisfied: mlxtend in ./.venv/lib/python3.12/site-packages
(0.23.3)
Requirement already satisfied: python-dateutil>=2.8.2 in
./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-
packages (from pandas) (2024.2)
Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-
packages (from pandas) (2024.2)
Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-
packages (from matplotlib) (1.3.1)
Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-
packages (from matplotlib) (0.12.1)
Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-
packages (from matplotlib) (4.55.2)
Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-
packages (from matplotlib) (1.4.7)
Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-
packages (from matplotlib) (24.2)
Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages
(from matplotlib) (11.0.0)
Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-
packages (from matplotlib) (3.2.0)
Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-
packages (from scikit-learn) (1.14.1)
Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-
packages (from scikit-learn) (1.4.2)
Requirement already satisfied: threadpoolctl>=3.1.0 in
./.venv/lib/python3.12/site-packages (from scikit-learn) (3.5.0)
Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages
(from python-dateutil>=2.8.2->pandas) (1.17.0)

\textbf{[}\textcolor{ansi-blue}{notice}\textbf{]} A new release of pip is
available: \textcolor{ansi-red}{24.2} -> \textcolor{ansi-green}{24.3.1}
\textbf{[}\textcolor{ansi-blue}{notice}\textbf{]} To update, run:
\textcolor{ansi-green}{pip install --upgrade pip}
Note: you may need to restart the kernel to use updated packages.
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{90}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{,} \PY{n}{permutation\PYZus{}test\PYZus{}score}\PY{p}{,} \PY{n}{StratifiedShuffleSplit}\PY{p}{,} \PY{n}{GridSearchCV}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{inspection} \PY{k+kn}{import} \PY{n}{permutation\PYZus{}importance}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k+kn}{import} \PY{n}{LinearRegression}\PY{p}{,} \PY{n}{LogisticRegression}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{feature\PYZus{}selection} \PY{k+kn}{import} \PY{n}{SequentialFeatureSelector}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{p}{(}
    \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{,} \PY{n}{mean\PYZus{}absolute\PYZus{}error}\PY{p}{,} \PY{n}{r2\PYZus{}score}\PY{p}{,}
    \PY{n}{roc\PYZus{}curve}\PY{p}{,} \PY{n}{auc}\PY{p}{,}
    \PY{n}{classification\PYZus{}report}\PY{p}{,} \PY{n}{accuracy\PYZus{}score}\PY{p}{,} \PY{n}{confusion\PYZus{}matrix}\PY{p}{,}
    \PY{n}{adjusted\PYZus{}rand\PYZus{}score}\PY{p}{,} \PY{n}{silhouette\PYZus{}score}
\PY{p}{)}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{mixture} \PY{k+kn}{import} \PY{n}{GaussianMixture}
\PY{k+kn}{from} \PY{n+nn}{mlxtend}\PY{n+nn}{.}\PY{n+nn}{evaluate} \PY{k+kn}{import} \PY{n}{bias\PYZus{}variance\PYZus{}decomp}

\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k+kn}{import} \PY{n}{PolynomialFeatures}\PY{p}{,} \PY{n}{StandardScaler}\PY{p}{,} \PY{n}{LabelEncoder}\PY{p}{,} \PY{n}{LabelBinarizer}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{pipeline} \PY{k+kn}{import} \PY{n}{make\PYZus{}pipeline}

\PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k+kn}{import} \PY{n}{tree}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{tree} \PY{k+kn}{import} \PY{n}{DecisionTreeClassifier}\PY{p}{,} \PY{n}{plot\PYZus{}tree}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k+kn}{import} \PY{n}{RandomForestClassifier}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neighbors} \PY{k+kn}{import} \PY{n}{KNeighborsClassifier}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k+kn}{import} \PY{n}{PCA}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{cluster} \PY{k+kn}{import} \PY{n}{KMeans}\PY{p}{,} \PY{n}{AgglomerativeClustering}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k+kn}{import} \PY{n}{OneHotEncoder}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{compose} \PY{k+kn}{import} \PY{n}{ColumnTransformer}

\PY{k+kn}{import} \PY{n+nn}{torch}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn} \PY{k}{as} \PY{n+nn}{nn}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{optim} \PY{k}{as} \PY{n+nn}{optim}
\PY{k+kn}{import} \PY{n+nn}{lightning} \PY{k}{as} \PY{n+nn}{pl}
\PY{k+kn}{import} \PY{n+nn}{lightning}\PY{n+nn}{.}\PY{n+nn}{pytorch}\PY{n+nn}{.}\PY{n+nn}{callbacks} \PY{k}{as} \PY{n+nn}{pl\PYZus{}callbacks}
\PY{k+kn}{from} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{utils}\PY{n+nn}{.}\PY{n+nn}{data} \PY{k+kn}{import} \PY{n}{DataLoader}\PY{p}{,} \PY{n}{Dataset}\PY{p}{,} \PY{n}{TensorDataset}
\PY{k+kn}{from} \PY{n+nn}{typing} \PY{k+kn}{import} \PY{n}{cast}

\PY{k+kn}{import} \PY{n+nn}{os}
\PY{n}{is\PYZus{}linux} \PY{o}{=} \PY{n}{os}\PY{o}{.}\PY{n}{name} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{posix}\PY{l+s+s1}{\PYZsq{}}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{52}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Reading in the data}
\PY{n}{spotify} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dataset.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{spotify}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{52}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
        Unnamed: 0                track\_id                 artists  \textbackslash{}
0                0  5SuOikwiRyPMVoIQDJUgSV             Gen Hoshino
1                1  4qPNDBW1i3p13qLCt0Ki3A            Ben Woodward
2                2  1iJBSr7s7jYXzM8EGcbK5b  Ingrid Michaelson;ZAYN
3                3  6lfxq3CG4xtTiEg7opyCyx            Kina Grannis
4                4  5vjLSffimiIP26QG5WcN2K        Chord Overstreet
{\ldots}            {\ldots}                     {\ldots}                     {\ldots}
113995      113995  2C3TZjDRiAzdyViavDJ217           Rainy Lullaby
113996      113996  1hIz5L4IB9hN3WRYPOCGPw           Rainy Lullaby
113997      113997  6x8ZfSoqDjuNa5SVP5QjvX           Cesária Evora
113998      113998  2e6sXL2bYv4bSz6VTdnfLs        Michael W. Smith
113999      113999  2hETkH7cOfqmz3LqZDHZf5           Cesária Evora

                                               album\_name  \textbackslash{}
0                                                  Comedy
1                                        Ghost (Acoustic)
2                                          To Begin Again
3       Crazy Rich Asians (Original Motion Picture Sou{\ldots}
4                                                 Hold On
{\ldots}                                                   {\ldots}
113995  \#mindfulness - Soft Rain for Mindful Meditatio{\ldots}
113996  \#mindfulness - Soft Rain for Mindful Meditatio{\ldots}
113997                                            Best Of
113998                                  Change Your World
113999                                     Miss Perfumado

                        track\_name  popularity  duration\_ms  explicit  \textbackslash{}
0                           Comedy          73       230666     False
1                 Ghost - Acoustic          55       149610     False
2                   To Begin Again          57       210826     False
3       Can't Help Falling In Love          71       201933     False
4                          Hold On          82       198853     False
{\ldots}                            {\ldots}         {\ldots}          {\ldots}       {\ldots}
113995         Sleep My Little Boy          21       384999     False
113996            Water Into Light          22       385000     False
113997              Miss Perfumado          22       271466     False
113998                     Friends          41       283893     False
113999                   Barbincor          22       241826     False

        danceability  energy  {\ldots}  loudness  mode  speechiness  acousticness  \textbackslash{}
0              0.676  0.4610  {\ldots}    -6.746     0       0.1430        0.0322
1              0.420  0.1660  {\ldots}   -17.235     1       0.0763        0.9240
2              0.438  0.3590  {\ldots}    -9.734     1       0.0557        0.2100
3              0.266  0.0596  {\ldots}   -18.515     1       0.0363        0.9050
4              0.618  0.4430  {\ldots}    -9.681     1       0.0526        0.4690
{\ldots}              {\ldots}     {\ldots}  {\ldots}       {\ldots}   {\ldots}          {\ldots}           {\ldots}
113995         0.172  0.2350  {\ldots}   -16.393     1       0.0422        0.6400
113996         0.174  0.1170  {\ldots}   -18.318     0       0.0401        0.9940
113997         0.629  0.3290  {\ldots}   -10.895     0       0.0420        0.8670
113998         0.587  0.5060  {\ldots}   -10.889     1       0.0297        0.3810
113999         0.526  0.4870  {\ldots}   -10.204     0       0.0725        0.6810

        instrumentalness  liveness  valence    tempo  time\_signature  \textbackslash{}
0               0.000001    0.3580   0.7150   87.917               4
1               0.000006    0.1010   0.2670   77.489               4
2               0.000000    0.1170   0.1200   76.332               4
3               0.000071    0.1320   0.1430  181.740               3
4               0.000000    0.0829   0.1670  119.949               4
{\ldots}                  {\ldots}       {\ldots}      {\ldots}      {\ldots}             {\ldots}
113995          0.928000    0.0863   0.0339  125.995               5
113996          0.976000    0.1050   0.0350   85.239               4
113997          0.000000    0.0839   0.7430  132.378               4
113998          0.000000    0.2700   0.4130  135.960               4
113999          0.000000    0.0893   0.7080   79.198               4

        track\_genre
0          acoustic
1          acoustic
2          acoustic
3          acoustic
4          acoustic
{\ldots}             {\ldots}
113995  world-music
113996  world-music
113997  world-music
113998  world-music
113999  world-music

[114000 rows x 21 columns]
\end{Verbatim}
\end{tcolorbox}
        
    We decided to remove the ``Unnamed: 0'' column, since we can uniquely
identify songs from their track\_id, so the counter values aren't very
useful.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{53}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{spotify\PYZus{}clean} \PY{o}{=} \PY{n}{spotify}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Unnamed: 0}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Then we looked into NA values, and found one observation. We decided to
drop this row since along with missing values, it also has other
unexpected/unreasonable statistics, such as popularity being 0, and a
duration of 0.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{54}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n}{spotify\PYZus{}clean}\PY{o}{.}\PY{n}{isna}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
track\_id            0
artists             1
album\_name          1
track\_name          1
popularity          0
duration\_ms         0
explicit            0
danceability        0
energy              0
key                 0
loudness            0
mode                0
speechiness         0
acousticness        0
instrumentalness    0
liveness            0
valence             0
tempo               0
time\_signature      0
track\_genre         0
dtype: int64
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{55}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Looking specifically at the row of missing values}
\PY{n}{spotify\PYZus{}clean}\PY{p}{[}\PY{n}{spotify\PYZus{}clean}\PY{o}{.}\PY{n}{isna}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{any}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{55}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                     track\_id artists album\_name track\_name  popularity  \textbackslash{}
65900  1kR4gIb7nGxHPI3D2ifs59     NaN        NaN        NaN           0

       duration\_ms  explicit  danceability  energy  key  loudness  mode  \textbackslash{}
65900            0     False         0.501   0.583    7     -9.46     0

       speechiness  acousticness  instrumentalness  liveness  valence  \textbackslash{}
65900       0.0605          0.69           0.00396    0.0747    0.734

         tempo  time\_signature track\_genre
65900  138.391               4       k-pop
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{56}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Dropping the row}
\PY{n}{spotify\PYZus{}clean} \PY{o}{=} \PY{n}{spotify\PYZus{}clean}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    To investigate any unreasonable/odd values, we decided to visualize each
numeric variable with boxplots.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{57}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} String and categorical columns in the dataset}
\PY{c+c1}{\PYZsh{} these will be dropped or one\PYZhy{}hot\PYZhy{}encoded depending on the model}
\PY{n}{string\PYZus{}columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{track\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artists}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{album\PYZus{}name}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{track\PYZus{}name}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{n}{categorical\PYZus{}columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{key}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mode}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{time\PYZus{}signature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{track\PYZus{}genre}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

\PY{c+c1}{\PYZsh{} helper function to remove all categorical and string columns}
\PY{k}{def} \PY{n+nf}{numeric}\PY{p}{(}\PY{n}{df}\PY{p}{:} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{)} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{:}
    \PY{n}{df\PYZus{}number} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{select\PYZus{}dtypes}\PY{p}{(}\PY{n}{include}\PY{o}{=}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{number}\PY{p}{]}\PY{p}{)}
    \PY{k}{return} \PY{n}{df\PYZus{}number}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{o}{\PYZti{}}\PY{n}{df\PYZus{}number}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{isin}\PY{p}{(}\PY{n}{categorical\PYZus{}columns}\PY{p}{)}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{58}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Creating boxplots}
\PY{n}{numeric\PYZus{}spotify} \PY{o}{=} \PY{n}{numeric}\PY{p}{(}\PY{n}{spotify\PYZus{}clean}\PY{p}{)}
\PY{n}{columns} \PY{o}{=} \PY{n}{numeric\PYZus{}spotify}\PY{o}{.}\PY{n}{columns}

\PY{n}{numeric\PYZus{}spotify}\PY{p}{[}\PY{n}{columns}\PY{p}{[}\PY{p}{:}\PY{n+nb}{len}\PY{p}{(}\PY{n}{columns}\PY{p}{)} \PY{o}{/}\PY{o}{/} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{o}{.}\PY{n}{box}\PY{p}{(}\PY{n}{figsize} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{14}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{,} \PY{n}{subplots}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{numeric\PYZus{}spotify}\PY{p}{[}\PY{n}{columns}\PY{p}{[}\PY{n+nb}{len}\PY{p}{(}\PY{n}{columns}\PY{p}{)} \PY{o}{/}\PY{o}{/} \PY{l+m+mi}{2}\PY{p}{:}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{o}{.}\PY{n}{box}\PY{p}{(}\PY{n}{figsize} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{14}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{,} \PY{n}{subplots}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{58}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
speechiness            Axes(0.125,0.11;0.110714x0.77)
acousticness        Axes(0.257857,0.11;0.110714x0.77)
instrumentalness    Axes(0.390714,0.11;0.110714x0.77)
liveness            Axes(0.523571,0.11;0.110714x0.77)
valence             Axes(0.656429,0.11;0.110714x0.77)
tempo               Axes(0.789286,0.11;0.110714x0.77)
dtype: object
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_14_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_14_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Based on the boxplots, there seem to be quite a few numeric variables
with a significant number of outliers. However, we decided to mainly
only focus on the songs in which these outliers would be unreasonable
based on domain knowledge, including duration being too low, the tempo
being too slow, and the time signature being 0.

First, we addressed the songs with duration being too low (the song is
too short). We decided that a threshold for a song to be ``too short''
as a song that is shorter than 30 seconds.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{59}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{short\PYZus{}songs} \PY{o}{=} \PY{n}{spotify\PYZus{}clean}\PY{p}{[}\PY{n}{spotify\PYZus{}clean}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{duration\PYZus{}ms}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZlt{}} \PY{l+m+mi}{30000}\PY{p}{]}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of tracks with duration less than 30 seconds: }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{len}\PY{p}{(}\PY{n}{short\PYZus{}songs}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{short\PYZus{}songs}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Number of tracks with duration less than 30 seconds: 16
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{59}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                      track\_id  \textbackslash{}
11398   1egJZfc8JBT2blFQ4clPKe
16288   1T5QvLF9lO4HO3OZQbaX9p
16292   5viwzFJxwRE1OEUR7G6hiD
16856   5YKCM3jbJ8lqUXUwfU7KwZ
39233   1T5QvLF9lO4HO3OZQbaX9p
39236   5viwzFJxwRE1OEUR7G6hiD
59306   3qSaeaXmtOuzkqe7DKgoiM
59310   6hsyfegVY5yklJneM40mWi
59434   1AsX7B48DFJZplJEwmhGpl
59458   1sayezH8bWoxHMAQCccCTi
59609   1oVrTBrCsM2eTE1G50yxY9
59711   787rIUDmWWBIfZXwHUeXXQ
59775   1HVjSh7scH1PaPiLjy2LEu
59812   38Ogh3rsHba83kXx13gbKs
66925   0DVMaexfdXDz19zUv1zKej
101159  6bg9fEHIulR3DYNYbWG0Jz

                                                  artists  \textbackslash{}
11398                    Benjamin Britten;Steven Isserlis
16288                    Robert Schumann;Pavel Nersessian
16292                    Robert Schumann;Pavel Nersessian
16856              Wolfgang Amadeus Mozart;Ingrid Haebler
39233                    Robert Schumann;Pavel Nersessian
39236                    Robert Schumann;Pavel Nersessian
59306                                          Leila Bela
59310                                          Leila Bela
59434   Alireza Mashayekhi;Ata Ebtekar;The Iranian Orc{\ldots}
59458                                          Leila Bela
59609                                          Leila Bela
59711                                          Leila Bela
59775                      Leila Bela;Leila's Opera Class
59812                                          Leila Bela
66925                                   Dora The Explorer
101159  Traditional;Cappella Musicale di Santa Maria i{\ldots}

                                               album\_name  \textbackslash{}
11398                          October Classical Playlist
16288   Schumann, Poulenc \& Others: Piano Works (Live {\ldots}
16292   Schumann, Poulenc \& Others: Piano Works (Live {\ldots}
16856                  Mozart: The Complete Piano Sonatas
39233   Schumann, Poulenc \& Others: Piano Works (Live {\ldots}
39236   Schumann, Poulenc \& Others: Piano Works (Live {\ldots}
59306                                         Angra Manyu
59310                                         Angra Manyu
59434                                       Ornamentalism
59458                                         Angra Manyu
59609                                         Angra Manyu
59711                                         Angra Manyu
59775                                         Angra Manyu
59812                                         Angra Manyu
66925                                   Dora The Explorer
101159       Giovannini: Messa a Quattro Breve Concertata

                                               track\_name  popularity  \textbackslash{}
11398   Cello Suite No. 3, Op. 87: IX. Passacaglia (Ex{\ldots}           0
16288   Carnaval, Op. 9: No. 20, Pause (Live in Japan,{\ldots}           0
16292   Carnaval, Op. 9: No. 13, Estrella (Live in Jap{\ldots}           0
16856                           Andante in C Major, K. 1a           0
39233   Carnaval, Op. 9: No. 20, Pause (Live in Japan,{\ldots}           0
39236   Carnaval, Op. 9: No. 13, Estrella (Live in Jap{\ldots}           0
59306                                                 V-7           0
59310                              The Exorsism Begins{\ldots}           0
59434                                          Aural Blue           0
59458                                                 V-3           0
59609                                             Shatter           0
59711                                       Breath Ritual           0
59775   Screams for a Finale! (feat. Leila's Opera Class)           0
59812                                                 V-4           0
66925                                 Backpack, Backpack!           8
101159                                         Pax Domini           0

        duration\_ms  explicit  danceability  energy  key  loudness  mode  \textbackslash{}
11398         22266     False         0.335  0.0593   11   -26.365     0
16288         17826     False         0.372  0.2780    8   -16.882     1
16292         23506     False         0.379  0.2370    5   -18.265     1
16856         17453     False         0.467  0.0301    2   -28.518     0
39233         17826     False         0.372  0.2780    8   -16.882     1
39236         23506     False         0.379  0.2370    5   -18.265     1
59306         21120     False         0.229  0.0577    8   -27.960     0
59310          8586     False         0.000  0.0400    8   -29.714     0
59434         24666     False         0.187  0.9750    1    -8.223     1
59458         28026     False         0.612  0.1370    8   -31.953     1
59609         21240     False         0.424  0.8690    9    -8.168     0
59711         28946     False         0.359  0.1430    4   -30.401     1
59775         15800     False         0.251  0.5080    5   -10.564     0
59812         13386     False         0.000  0.2240   11   -22.196     1
66925         24266     False         0.903  0.3940    2    -8.018     1
101159        24000     False         0.358  0.0335    1   -28.683     1

        speechiness  acousticness  instrumentalness  liveness  valence  \textbackslash{}
11398        0.0430        0.9920            0.8690    0.1160   0.1950
16288        0.0370        0.9850            0.9210    0.1640   0.9120
16292        0.0470        0.9930            0.8870    0.1440   0.4770
16856        0.0428        0.9950            0.9000    0.1240   0.0000
39233        0.0370        0.9850            0.9210    0.1640   0.9120
39236        0.0470        0.9930            0.8870    0.1440   0.4770
59306        0.1960        0.6260            0.9310    0.1080   0.2530
59310        0.0000        0.9280            0.9560    0.1150   0.0000
59434        0.2360        0.0431            0.9800    0.3570   0.1360
59458        0.7920        0.8480            0.0000    0.0868   0.3930
59609        0.0728        0.7070            0.0893    0.1170   0.0000
59711        0.4390        0.3930            0.0379    0.3930   0.0366
59775        0.3160        0.9690            0.9990    0.9520   0.0000
59812        0.0000        0.9700            0.0000    0.9070   0.0000
66925        0.1040        0.0047            0.0000    0.0830   0.5430
101159       0.0435        0.9810            0.0000    0.2510   0.2980

          tempo  time\_signature track\_genre
11398    77.266               5     british
16288    89.032               1   classical
16292   116.093               4   classical
16856    84.375               4   classical
39233    89.032               1      german
39236   116.093               4      german
59306   172.897               4     iranian
59310     0.000               0     iranian
59434    96.548               4     iranian
59458   100.765               4     iranian
59609   135.107               4     iranian
59711   119.207               5     iranian
59775   184.051               3     iranian
59812     0.000               0     iranian
66925   109.974               4        kids
101159   95.684               3       sleep
\end{Verbatim}
\end{tcolorbox}
        
    We found that there are only 16 songs that are less than 30 seconds.
Since there were so few, we decided to look manually into these songs
and investigate whether or not these songs seem to be real songs. Turns
out, the majority of them are concert pauses (empty tracks) or sound
effects and sound clips. Since we don't want these tracks to affect our
investigation of genres, we decided to drop them.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{60}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{spotify\PYZus{}clean} \PY{o}{=} \PY{n}{spotify\PYZus{}clean}\PY{p}{[}\PY{n}{spotify\PYZus{}clean}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{duration\PYZus{}ms}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mi}{30000}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    Next we looked at implausible 0 values with time signature and tempo.
For context, time signature should be expressed in a fraction, while
tempo is in BPM (so 0 BPM doesn't make sense).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{61}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Tempos == 0: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{spotify\PYZus{}clean}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tempo}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{eq}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Time signatures == 0: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{spotify\PYZus{}clean}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{time\PYZus{}signature}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{eq}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Tempos == 0:  155
Time signatures == 0:  161
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{62}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{tempo\PYZus{}timesig\PYZus{}0\PYZus{}songs} \PY{o}{=} \PY{n}{spotify\PYZus{}clean}\PY{p}{[}\PY{p}{(}\PY{n}{spotify\PYZus{}clean}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tempo}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)} \PY{o}{|} \PY{p}{(}\PY{n}{spotify\PYZus{}clean}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{time\PYZus{}signature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{]}
\PY{n}{tempo\PYZus{}timesig\PYZus{}0\PYZus{}songs}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{62}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                      track\_id  \textbackslash{}
2926    0jdfbvSdaWvxfAlD20TtNc
4131    59gg6zQhSKGVnkT3hWAY3l
4379    4acmzQsAeMJa5sGFSog7fu
4664    1Kb2DqjHRvOcT5xeWtz3t5
26910   7HSc2wpHlXKIl8SCZK7zsP
{\ldots}                        {\ldots}
101993  6H0kAiSAFB84jX7dgEDWd6
112172  0jdfbvSdaWvxfAlD20TtNc
113428  5EYzrykQ95uOmepteDi9KT
113688  2EnZf7wbFv7ST4CJ3EvNzT
113856  6XsYJ0dwT2hRzp0Qles78F

                                                  artists  \textbackslash{}
2926                                           Yaşlı Amca
4131                                Max Richter;Lang Lang
4379     Dario Marianelli;Jack Liebeck;Benjamin Wallfisch
4664                                     Sylvain Chauveau
26910                                        Benny Martin
{\ldots}                                                   {\ldots}
101993                                        Rain Sounds
112172                                         Yaşlı Amca
113428  El Ruido Blanco;Soñoliento Juan;Mantra para Do{\ldots}
113688                                    El Ruido Blanco
113856                                  Колыбельная-земля

                                               album\_name  \textbackslash{}
2926                                            Akşamüstü
4131                      Voyager - Essential Max Richter
4379       Jane Eyre - Original Motion Picture Soundtrack
4664                              Des Plumes Dans La Tête
26910             Here Comes the Sun (Piano Instrumental)
{\ldots}                                                   {\ldots}
101993                                               Rain
112172                                          Akşamüstü
113428                 Aire Acondicionado de Ruido Blanco
113688  Ruido Blanco para el bebé: sonidos relajantes {\ldots}
113856  Расслабляющие звуки - белый шум для вашего реб{\ldots}

                                       track\_name  popularity  duration\_ms  \textbackslash{}
2926                              Sanki Yapamadım          44       213198
4131                                The Departure          64       151506
4379    The End of Childhood (feat. Jack Liebeck)          55        73266
4664                               Ferme Les Yeux          53        68493
26910     Here Comes the Sun (Piano Instrumental)          18       203705
{\ldots}                                           {\ldots}         {\ldots}          {\ldots}
101993                    Rain: Natural Recording          32        84219
112172                            Sanki Yapamadım          44       213198
113428                             Aire de verano          27       128000
113688         Ruido Rosa Puro - Una Hora Versión          24      3601693
113856                        Пылесос (Белый шум)          22       302185

        explicit  danceability   energy  key  loudness  mode  speechiness  \textbackslash{}
2926       False         0.442  0.56700    8    -6.346     0       0.0516
4131       False         0.000  0.03620    0   -22.519     0       0.0000
4379       False         0.000  0.04450    0   -26.440     0       0.0000
4664       False         0.000  0.03230    2   -23.636     0       0.0000
26910      False         0.329  0.06070    9   -28.310     1       0.0507
{\ldots}          {\ldots}           {\ldots}      {\ldots}  {\ldots}       {\ldots}   {\ldots}          {\ldots}
101993     False         0.000  0.02540    8   -19.925     1       0.0000
112172     False         0.442  0.56700    8    -6.346     0       0.0516
113428     False         0.000  0.18800    8   -25.837     0       0.0000
113688     False         0.000  0.00002    1   -11.165     1       0.0000
113856     False         0.000  0.22400    8   -10.224     1       0.0000

        acousticness  instrumentalness  liveness  valence    tempo  \textbackslash{}
2926        0.238000          0.000325    0.0852    0.639  138.616
4131        0.994000          0.940000    0.0958    0.000    0.000
4379        0.972000          0.972000    0.0873    0.000    0.000
4664        0.994000          0.973000    0.0922    0.000    0.000
26910       0.994000          0.880000    0.0858    0.421   93.948
{\ldots}              {\ldots}               {\ldots}       {\ldots}      {\ldots}      {\ldots}
101993      0.000002          0.838000    0.3390    0.000    0.000
112172      0.238000          0.000325    0.0852    0.639  138.616
113428      0.139000          0.339000    0.1220    0.000    0.000
113688      0.186000          1.000000    0.3620    0.000    0.000
113856      0.142000          0.986000    0.4110    0.000    0.000

        time\_signature  track\_genre
2926                 0     alt-rock
4131                 0      ambient
4379                 0      ambient
4664                 0      ambient
26910                0       disney
{\ldots}                {\ldots}          {\ldots}
101993               0        sleep
112172               0      turkish
113428               0  world-music
113688               0  world-music
113856               0  world-music

[161 rows x 20 columns]
\end{Verbatim}
\end{tcolorbox}
        
    We manually looked through the songs with time signature and tempos of
0, and found that most of them tended to be ambient tracks, such as
tracks of rainfall sounds or white noise. We decided that it might be
good to remove these rows from our cleaned dataset, as we didn't want to
the zero values to skew the rest of our exploration in any way, and also
because we hoped to look more a typical songs rather than ambient
tracks.

Additionally, the overall count of these suspicious rows wasn't very
high compared to size of the entire dataset (163 vs \textgreater{}
114k), so we figured that removing these values wouldn't drastically
affect the size and comprehensiveness of our dataset.

We also confirmed this decision by seeing how the means of some of the
other variables would change based on if we removed or kept these
suspicious rows (see below).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{63}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{spotify\PYZus{}clean\PYZus{}temp} \PY{o}{=} \PY{n}{spotify\PYZus{}clean}\PY{p}{[}\PY{n}{spotify\PYZus{}clean}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{time\PYZus{}signature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{!=} \PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{spotify\PYZus{}clean\PYZus{}temp} \PY{o}{=} \PY{n}{spotify\PYZus{}clean}\PY{p}{[}\PY{n}{spotify\PYZus{}clean}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tempo}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{!=} \PY{l+m+mi}{0}\PY{p}{]}

\PY{n}{mean\PYZus{}temp} \PY{o}{=} \PY{n}{spotify\PYZus{}clean\PYZus{}temp}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{numeric\PYZus{}only}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{mean\PYZus{}clean} \PY{o}{=} \PY{n}{spotify\PYZus{}clean}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{numeric\PYZus{}only}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Calculate the difference}
\PY{n}{mean\PYZus{}difference} \PY{o}{=} \PY{n}{mean\PYZus{}clean} \PY{o}{\PYZhy{}} \PY{n}{mean\PYZus{}temp}
\PY{n}{mean\PYZus{}difference}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{63}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
popularity           0.006038
duration\_ms        -73.741857
explicit            -0.000116
danceability        -0.000772
energy              -0.000706
key                 -0.000157
loudness            -0.018772
mode                 0.000072
speechiness         -0.000115
acousticness         0.000240
instrumentalness     0.000676
liveness             0.000327
valence             -0.000646
tempo               -0.166333
time\_signature      -0.005316
dtype: float64
\end{Verbatim}
\end{tcolorbox}
        
    From this, the effects on the means by removing the suspicious rows
doesn't seem to be too large, with the largest being duration, which is
measured in milliseconds, so would actually be less than a second
difference. Thus, we dropped the rows where time signature or tempo were
0, which was a total of 161 rows.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{64}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{spotify\PYZus{}clean} \PY{o}{=} \PY{n}{spotify\PYZus{}clean\PYZus{}temp}
\PY{n}{spotify\PYZus{}clean}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{64}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                      track\_id                 artists  \textbackslash{}
0       5SuOikwiRyPMVoIQDJUgSV             Gen Hoshino
1       4qPNDBW1i3p13qLCt0Ki3A            Ben Woodward
2       1iJBSr7s7jYXzM8EGcbK5b  Ingrid Michaelson;ZAYN
3       6lfxq3CG4xtTiEg7opyCyx            Kina Grannis
4       5vjLSffimiIP26QG5WcN2K        Chord Overstreet
{\ldots}                        {\ldots}                     {\ldots}
113995  2C3TZjDRiAzdyViavDJ217           Rainy Lullaby
113996  1hIz5L4IB9hN3WRYPOCGPw           Rainy Lullaby
113997  6x8ZfSoqDjuNa5SVP5QjvX           Cesária Evora
113998  2e6sXL2bYv4bSz6VTdnfLs        Michael W. Smith
113999  2hETkH7cOfqmz3LqZDHZf5           Cesária Evora

                                               album\_name  \textbackslash{}
0                                                  Comedy
1                                        Ghost (Acoustic)
2                                          To Begin Again
3       Crazy Rich Asians (Original Motion Picture Sou{\ldots}
4                                                 Hold On
{\ldots}                                                   {\ldots}
113995  \#mindfulness - Soft Rain for Mindful Meditatio{\ldots}
113996  \#mindfulness - Soft Rain for Mindful Meditatio{\ldots}
113997                                            Best Of
113998                                  Change Your World
113999                                     Miss Perfumado

                        track\_name  popularity  duration\_ms  explicit  \textbackslash{}
0                           Comedy          73       230666     False
1                 Ghost - Acoustic          55       149610     False
2                   To Begin Again          57       210826     False
3       Can't Help Falling In Love          71       201933     False
4                          Hold On          82       198853     False
{\ldots}                            {\ldots}         {\ldots}          {\ldots}       {\ldots}
113995         Sleep My Little Boy          21       384999     False
113996            Water Into Light          22       385000     False
113997              Miss Perfumado          22       271466     False
113998                     Friends          41       283893     False
113999                   Barbincor          22       241826     False

        danceability  energy  key  loudness  mode  speechiness  acousticness  \textbackslash{}
0              0.676  0.4610    1    -6.746     0       0.1430        0.0322
1              0.420  0.1660    1   -17.235     1       0.0763        0.9240
2              0.438  0.3590    0    -9.734     1       0.0557        0.2100
3              0.266  0.0596    0   -18.515     1       0.0363        0.9050
4              0.618  0.4430    2    -9.681     1       0.0526        0.4690
{\ldots}              {\ldots}     {\ldots}  {\ldots}       {\ldots}   {\ldots}          {\ldots}           {\ldots}
113995         0.172  0.2350    5   -16.393     1       0.0422        0.6400
113996         0.174  0.1170    0   -18.318     0       0.0401        0.9940
113997         0.629  0.3290    0   -10.895     0       0.0420        0.8670
113998         0.587  0.5060    7   -10.889     1       0.0297        0.3810
113999         0.526  0.4870    1   -10.204     0       0.0725        0.6810

        instrumentalness  liveness  valence    tempo  time\_signature  \textbackslash{}
0               0.000001    0.3580   0.7150   87.917               4
1               0.000006    0.1010   0.2670   77.489               4
2               0.000000    0.1170   0.1200   76.332               4
3               0.000071    0.1320   0.1430  181.740               3
4               0.000000    0.0829   0.1670  119.949               4
{\ldots}                  {\ldots}       {\ldots}      {\ldots}      {\ldots}             {\ldots}
113995          0.928000    0.0863   0.0339  125.995               5
113996          0.976000    0.1050   0.0350   85.239               4
113997          0.000000    0.0839   0.7430  132.378               4
113998          0.000000    0.2700   0.4130  135.960               4
113999          0.000000    0.0893   0.7080   79.198               4

        track\_genre
0          acoustic
1          acoustic
2          acoustic
3          acoustic
4          acoustic
{\ldots}             {\ldots}
113995  world-music
113996  world-music
113997  world-music
113998  world-music
113999  world-music

[113828 rows x 20 columns]
\end{Verbatim}
\end{tcolorbox}
        
    Since there don't seem to be anymore values in our dataset that are
unreasonable or missing, we exported our cleaned dataset to start
exploring and visualizing our data.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{65}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Exporting our cleaned dataset for analysis}
\PY{n}{spotify\PYZus{}clean}\PY{o}{.}\PY{n}{to\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{spotify.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Exploratory Data Analysis
(EDA)}\label{exploratory-data-analysis-eda}

In order to find out a little more about the general structure and
features of our dataset, we did some exploratory data analysis. We
started by looking at the shape and structure of the data, and then the
correlations between the numeric values, and then also did some
visualizations of categorical features.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{66}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Read in the cleaned data}
\PY{n}{spotify} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{spotify.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{spotify}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{66}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                      track\_id                 artists  \textbackslash{}
0       5SuOikwiRyPMVoIQDJUgSV             Gen Hoshino
1       4qPNDBW1i3p13qLCt0Ki3A            Ben Woodward
2       1iJBSr7s7jYXzM8EGcbK5b  Ingrid Michaelson;ZAYN
3       6lfxq3CG4xtTiEg7opyCyx            Kina Grannis
4       5vjLSffimiIP26QG5WcN2K        Chord Overstreet
{\ldots}                        {\ldots}                     {\ldots}
113823  2C3TZjDRiAzdyViavDJ217           Rainy Lullaby
113824  1hIz5L4IB9hN3WRYPOCGPw           Rainy Lullaby
113825  6x8ZfSoqDjuNa5SVP5QjvX           Cesária Evora
113826  2e6sXL2bYv4bSz6VTdnfLs        Michael W. Smith
113827  2hETkH7cOfqmz3LqZDHZf5           Cesária Evora

                                               album\_name  \textbackslash{}
0                                                  Comedy
1                                        Ghost (Acoustic)
2                                          To Begin Again
3       Crazy Rich Asians (Original Motion Picture Sou{\ldots}
4                                                 Hold On
{\ldots}                                                   {\ldots}
113823  \#mindfulness - Soft Rain for Mindful Meditatio{\ldots}
113824  \#mindfulness - Soft Rain for Mindful Meditatio{\ldots}
113825                                            Best Of
113826                                  Change Your World
113827                                     Miss Perfumado

                        track\_name  popularity  duration\_ms  explicit  \textbackslash{}
0                           Comedy          73       230666     False
1                 Ghost - Acoustic          55       149610     False
2                   To Begin Again          57       210826     False
3       Can't Help Falling In Love          71       201933     False
4                          Hold On          82       198853     False
{\ldots}                            {\ldots}         {\ldots}          {\ldots}       {\ldots}
113823         Sleep My Little Boy          21       384999     False
113824            Water Into Light          22       385000     False
113825              Miss Perfumado          22       271466     False
113826                     Friends          41       283893     False
113827                   Barbincor          22       241826     False

        danceability  energy  key  loudness  mode  speechiness  acousticness  \textbackslash{}
0              0.676  0.4610    1    -6.746     0       0.1430        0.0322
1              0.420  0.1660    1   -17.235     1       0.0763        0.9240
2              0.438  0.3590    0    -9.734     1       0.0557        0.2100
3              0.266  0.0596    0   -18.515     1       0.0363        0.9050
4              0.618  0.4430    2    -9.681     1       0.0526        0.4690
{\ldots}              {\ldots}     {\ldots}  {\ldots}       {\ldots}   {\ldots}          {\ldots}           {\ldots}
113823         0.172  0.2350    5   -16.393     1       0.0422        0.6400
113824         0.174  0.1170    0   -18.318     0       0.0401        0.9940
113825         0.629  0.3290    0   -10.895     0       0.0420        0.8670
113826         0.587  0.5060    7   -10.889     1       0.0297        0.3810
113827         0.526  0.4870    1   -10.204     0       0.0725        0.6810

        instrumentalness  liveness  valence    tempo  time\_signature  \textbackslash{}
0               0.000001    0.3580   0.7150   87.917               4
1               0.000006    0.1010   0.2670   77.489               4
2               0.000000    0.1170   0.1200   76.332               4
3               0.000071    0.1320   0.1430  181.740               3
4               0.000000    0.0829   0.1670  119.949               4
{\ldots}                  {\ldots}       {\ldots}      {\ldots}      {\ldots}             {\ldots}
113823          0.928000    0.0863   0.0339  125.995               5
113824          0.976000    0.1050   0.0350   85.239               4
113825          0.000000    0.0839   0.7430  132.378               4
113826          0.000000    0.2700   0.4130  135.960               4
113827          0.000000    0.0893   0.7080   79.198               4

        track\_genre
0          acoustic
1          acoustic
2          acoustic
3          acoustic
4          acoustic
{\ldots}             {\ldots}
113823  world-music
113824  world-music
113825  world-music
113826  world-music
113827  world-music

[113828 rows x 20 columns]
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{67}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{spotify}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{67}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
          popularity   duration\_ms   danceability         energy  \textbackslash{}
count  113828.000000  1.138280e+05  113828.000000  113828.000000
mean       33.237384  2.281340e+05       0.567603       0.642140
std        22.314498  1.062882e+05       0.172371       0.250761
min         0.000000  3.008000e+04       0.051300       0.000020
25\%        17.000000  1.742130e+05       0.456000       0.473000
50\%        35.000000  2.130000e+05       0.580000       0.685000
75\%        50.000000  2.615970e+05       0.695000       0.854000
max       100.000000  5.237295e+06       0.985000       1.000000

                 key       loudness           mode    speechiness  \textbackslash{}
count  113828.000000  113828.000000  113828.000000  113828.000000
mean        5.309186      -8.238433       0.637488       0.084758
std         3.559470       4.991371       0.480728       0.105735
min         0.000000     -46.591000       0.000000       0.022100
25\%         2.000000     -10.000250       0.000000       0.035900
50\%         5.000000      -6.997000       1.000000       0.049000
75\%         8.000000      -5.000000       1.000000       0.084600
max        11.000000       4.532000       1.000000       0.965000

        acousticness  instrumentalness       liveness        valence  \textbackslash{}
count  113828.000000     113828.000000  113828.000000  113828.000000
mean        0.314602          0.155314       0.213220       0.474737
std         0.332302          0.308835       0.189925       0.258831
min         0.000000          0.000000       0.009250       0.000000
25\%         0.016900          0.000000       0.098000       0.261000
50\%         0.168000          0.000041       0.132000       0.464000
75\%         0.597000          0.047600       0.273000       0.683000
max         0.996000          1.000000       1.000000       0.995000

               tempo  time\_signature
count  113828.000000   113828.000000
mean      122.317259        3.909460
std        29.653245        0.407685
min        30.200000        0.000000
25\%        99.432750        4.000000
50\%       122.024000        4.000000
75\%       140.078000        4.000000
max       243.372000        5.000000
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{68}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Select only numeric columns (remove categorical ones) using numeric() function written in data cleaning section}
\PY{c+c1}{\PYZsh{} Removed categorical columns: key, time signature, mode}
\PY{n}{numeric\PYZus{}spotify} \PY{o}{=} \PY{n}{numeric}\PY{p}{(}\PY{n}{spotify}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    We started by looking at the distributions of all of the numeric
variables with a histogram.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{69}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Create subplots}
\PY{n}{num\PYZus{}columns} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{numeric\PYZus{}spotify}\PY{o}{.}\PY{n}{columns}\PY{p}{)}
\PY{n}{num\PYZus{}rows} \PY{o}{=} \PY{p}{(}\PY{n}{num\PYZus{}columns} \PY{o}{/}\PY{o}{/} \PY{l+m+mi}{3}\PY{p}{)} \PY{o}{+} \PY{p}{(}\PY{n}{num\PYZus{}columns} \PY{o}{\PYZpc{}} \PY{l+m+mi}{3} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{num\PYZus{}rows}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{num\PYZus{}rows} \PY{o}{*} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
\PY{n}{axes} \PY{o}{=} \PY{n}{axes}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Create a histogram for each variable}
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{column} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{numeric\PYZus{}spotify}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{p}{:}
    \PY{n}{ax} \PY{o}{=} \PY{n}{axes}\PY{p}{[}\PY{n}{i}\PY{p}{]}
    \PY{n}{numeric\PYZus{}spotify}\PY{p}{[}\PY{n}{column}\PY{p}{]}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{bins}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{edgecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{ax}\PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Histogram of }\PY{l+s+si}{\PYZob{}}\PY{n}{column}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Frequency}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{False}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Hiding unused subplots}
\PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{i} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{axes}\PY{p}{)}\PY{p}{)}\PY{p}{:}
    \PY{n}{axes}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{off}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_33_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Based on this, we definitely have a few variables that are extremely
skewed. However, we decided not to transform the data to make it more
normal because the most of the data is already on a scale between 0 and
1, and we didn't want to lose possible meaningful information. For
example, in the description of the data, it describes the
``speechiness'' variable with ``Values above 0.66 describe tracks that
are probably made entirely of spoken words. Values between 0.33 and 0.66
describe tracks that may contain both music and speech, either in
sections or layered, including such cases as rap music. Values below
0.33 most likely represent music and other non-speech-like tracks'',
which would be lost if we were to correct the right-skew of the
speechiness variable.

We then decided to try to look at some relationships between variables
by creating a correlation heatmap of every quantitative variable.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{70}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Finding the correlation matrix}
\PY{n}{spotify\PYZus{}corr} \PY{o}{=} \PY{n}{numeric\PYZus{}spotify}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}
\PY{n}{spotify\PYZus{}corr}

\PY{c+c1}{\PYZsh{} Making correlation heatmap}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
\PY{n}{sns}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{spotify\PYZus{}corr}\PY{p}{,} \PY{n}{annot}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{fmt}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.2f}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{coolwarm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidths}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_35_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Based on the heatmap of correlations, we wanted to see a more detailed
scatterplots of the features that were more correlated. So, we decided
to plot more detailed scatterplots of the correlations larger than 0.4
or smaller than -0.4. We then plotted a linear and quadratic regression
for each.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{71}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Filtering out the pairs of higher correlated features}
\PY{n}{correlated\PYZus{}features} \PY{o}{=} \PY{p}{[}
    \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{loudness}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{energy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
    \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{acousticness}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{energy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
    \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{acousticness}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{loudness}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
    \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{instrumentalness}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{loudness}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
    \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{valence}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{danceability}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
\PY{p}{]}

\PY{c+c1}{\PYZsh{} Making scatterplots for each variable vs popularity}
\PY{k}{for} \PY{p}{(}\PY{n}{i}\PY{p}{,} \PY{n}{j}\PY{p}{)} \PY{o+ow}{in} \PY{n}{correlated\PYZus{}features}\PY{p}{:}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{numeric\PYZus{}spotify}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{poly1d}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{polyfit}\PY{p}{(}\PY{n}{numeric\PYZus{}spotify}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{numeric\PYZus{}spotify}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{numeric\PYZus{}spotify}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{numeric\PYZus{}spotify}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{poly1d}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{polyfit}\PY{p}{(}\PY{n}{numeric\PYZus{}spotify}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{numeric\PYZus{}spotify}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{numeric\PYZus{}spotify}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{orange}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{numeric\PYZus{}spotify}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{numeric\PYZus{}spotify}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{n}{i}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{n}{j}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_37_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_37_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_37_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_37_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_37_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Though the variability from our fitted lines seemed pretty high on our
scatterplots, some of the shapes of the graphs were pretty interesting.
In particular, we thought the shapes of the (acousticness vs loudness),
(instrumentalness vs loudness), (acousticness vs energy), and (energy vs
loudness) graphs seemed to have a pretty distinct shape. The variables
may have some sort of relationship with each other, which we noted in
case they played some part in later sections of our exploration in our
project.

We also looked into the categorical variables (excluding genre, which we
will go into a more detailed exploration of later).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{72}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Selecting the categories: key, mode, explicit, time signature}
\PY{n}{spotify\PYZus{}categories} \PY{o}{=} \PY{n}{spotify}\PY{p}{[}\PY{n}{categorical\PYZus{}columns} \PY{o}{+} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{explicit}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}
\PY{n}{spotify\PYZus{}categories} \PY{o}{=} \PY{n}{spotify\PYZus{}categories}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{track\PYZus{}genre}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{key\PYZus{}mapping} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+m+mi}{0}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C♯/D♭}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{D}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{D♯/E♭}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{E}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F♯/G♭}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{G}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{l+m+mi}{8}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{G♯/A♭}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A♯/B♭}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{11}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{B}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{No Key Detected}\PY{l+s+s1}{\PYZsq{}}
\PY{p}{\PYZcb{}}

\PY{c+c1}{\PYZsh{} Map the \PYZsq{}key\PYZsq{} column to pitch names}
\PY{n}{spotify\PYZus{}categories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{key}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{spotify\PYZus{}categories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{key}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{map}\PY{p}{(}\PY{n}{key\PYZus{}mapping}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Mode is 0 or 1 (1 for Major, 0 for Minor)}
\PY{n}{spotify\PYZus{}categories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mode}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{spotify\PYZus{}categories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mode}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{map}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+m+mi}{0}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Minor}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Major}\PY{l+s+s1}{\PYZsq{}}\PY{p}{\PYZcb{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Time signature}
\PY{n}{spotify\PYZus{}categories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{time\PYZus{}signature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{spotify\PYZus{}categories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{time\PYZus{}signature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{str}\PY{p}{)}

\PY{n}{spotify\PYZus{}categories}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{72}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
          key   mode time\_signature  explicit
0       C♯/D♭  Minor              4     False
1       C♯/D♭  Major              4     False
2           C  Major              4     False
3           C  Major              3     False
4           D  Major              4     False
{\ldots}       {\ldots}    {\ldots}            {\ldots}       {\ldots}
113823      F  Major              5     False
113824      C  Minor              4     False
113825      C  Minor              4     False
113826      G  Major              4     False
113827  C♯/D♭  Minor              4     False

[113828 rows x 4 columns]
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{73}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Making barplots for each categorical value}

\PY{c+c1}{\PYZsh{} Create subplots for each categorical variable}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\PY{n}{num\PYZus{}columns} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{spotify\PYZus{}categories}\PY{o}{.}\PY{n}{columns}\PY{p}{)}
\PY{n}{num\PYZus{}rows} \PY{o}{=} \PY{p}{(}\PY{n}{num\PYZus{}columns} \PY{o}{/}\PY{o}{/} \PY{l+m+mi}{2}\PY{p}{)} \PY{o}{+} \PY{p}{(}\PY{n}{num\PYZus{}columns} \PY{o}{\PYZpc{}} \PY{l+m+mi}{2} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{num\PYZus{}rows}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{n}{num\PYZus{}rows} \PY{o}{*} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
\PY{n}{axes} \PY{o}{=} \PY{n}{axes}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Loop through each categorical variable to create a bar plot}
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{column} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{spotify\PYZus{}categories}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{p}{:}
    \PY{n}{ax} \PY{o}{=} \PY{n}{axes}\PY{p}{[}\PY{n}{i}\PY{p}{]}
    \PY{n}{sns}\PY{o}{.}\PY{n}{countplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{n}{column}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{spotify\PYZus{}categories}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{ax}\PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Distribution of }\PY{l+s+si}{\PYZob{}}\PY{n}{column}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{n}{column}\PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{False}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Hiding unused subplots}
\PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{i} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{axes}\PY{p}{)}\PY{p}{)}\PY{p}{:}
    \PY{n}{axes}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{off}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
<Figure size 1200x1000 with 0 Axes>
    \end{Verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_40_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Based on these visualizations it seems like the distributions for the
key the song is in is fairly uniformly distributed, with D\#/E♭ the
least number of tracks. Additionally, most songs seem to be in major,
have a 4/4 time signature, or non-explicit.

After a more general exploration of some of the relationships and other
features in our dataset, we decided to look closer into genre, the main
focus of our project.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{74}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{genre\PYZus{}counts} \PY{o}{=} \PY{n}{spotify}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{track\PYZus{}genre}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{genre\PYZus{}counts}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
track\_genre
acoustic       1000
afrobeat       1000
alt-rock       1000
alternative    1000
anime          1000
               {\ldots}
classical       997
world-music     997
guitar          996
iranian         990
sleep           861
Name: count, Length: 114, dtype: int64
    \end{Verbatim}

    We have 114 different genres, and they all seem fairly split. In fact,
our dataset seems to have 1000 samples for each genre, as the genres
that have less than 1000 samples are genres that we cleaned data values
from. We looked further into the specific genres and names.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{75}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{genre\PYZus{}names} \PY{o}{=} \PY{n}{spotify}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{track\PYZus{}genre}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{genre\PYZus{}names}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
['acoustic' 'afrobeat' 'alt-rock' 'alternative' 'ambient' 'anime'
 'black-metal' 'bluegrass' 'blues' 'brazil' 'breakbeat' 'british'
 'cantopop' 'chicago-house' 'children' 'chill' 'classical' 'club' 'comedy'
 'country' 'dance' 'dancehall' 'death-metal' 'deep-house' 'detroit-techno'
 'disco' 'disney' 'drum-and-bass' 'dub' 'dubstep' 'edm' 'electro'
 'electronic' 'emo' 'folk' 'forro' 'french' 'funk' 'garage' 'german'
 'gospel' 'goth' 'grindcore' 'groove' 'grunge' 'guitar' 'happy'
 'hard-rock' 'hardcore' 'hardstyle' 'heavy-metal' 'hip-hop' 'honky-tonk'
 'house' 'idm' 'indian' 'indie-pop' 'indie' 'industrial' 'iranian'
 'j-dance' 'j-idol' 'j-pop' 'j-rock' 'jazz' 'k-pop' 'kids' 'latin'
 'latino' 'malay' 'mandopop' 'metal' 'metalcore' 'minimal-techno' 'mpb'
 'new-age' 'opera' 'pagode' 'party' 'piano' 'pop-film' 'pop' 'power-pop'
 'progressive-house' 'psych-rock' 'punk-rock' 'punk' 'r-n-b' 'reggae'
 'reggaeton' 'rock-n-roll' 'rock' 'rockabilly' 'romance' 'sad' 'salsa'
 'samba' 'sertanejo' 'show-tunes' 'singer-songwriter' 'ska' 'sleep'
 'songwriter' 'soul' 'spanish' 'study' 'swedish' 'synth-pop' 'tango'
 'techno' 'trance' 'trip-hop' 'turkish' 'world-music']
    \end{Verbatim}

    We saw that some genres were a lot more specific than others, for
example, the genre of ``pop'' versus ``disney'', so we kept that in mind
while doing later investigations and processing that we might want to
group certain similar genres together, like grouping rock genres, pop
genres, jazz, genres, etc.

We also looked at how some of the other variables were distributed
grouped by track genre to see if certain genres had differentiable
characteristics.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{76}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{temp} \PY{o}{=} \PY{n}{spotify}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{track\PYZus{}genre}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{n}{by\PYZus{}genres} \PY{o}{=} \PY{n}{numeric}\PY{p}{(}\PY{n}{spotify}\PY{p}{)}
\PY{n}{by\PYZus{}genres}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{track\PYZus{}genre}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{temp}

\PY{n}{by\PYZus{}genres}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{track\PYZus{}genre}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{76}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
             popularity    duration\_ms  danceability    energy   loudness  \textbackslash{}
track\_genre
acoustic      42.483000  214896.957000      0.549593  0.435368  -9.447843
afrobeat      24.399000  248412.791000      0.669580  0.702812  -7.789353
alt-rock      33.943000  235455.907000      0.534493  0.754173  -6.191489
alternative   24.337000  222016.180000      0.559927  0.720030  -6.078777
ambient       44.151454  237478.207623      0.368974  0.237762 -18.584524
{\ldots}                 {\ldots}            {\ldots}           {\ldots}       {\ldots}        {\ldots}
techno        39.042000  312311.477000      0.684348  0.746413  -8.077874
trance        37.635000  269007.478000      0.583409  0.845272  -6.329711
trip-hop      34.460000  274954.026000      0.634695  0.622363  -9.239915
turkish       40.698000  219529.010000      0.616077  0.609804  -8.224722
world-music   41.925777  294045.881645      0.415819  0.534177  -9.398517

             speechiness  acousticness  instrumentalness  liveness   valence  \textbackslash{}
track\_genre
acoustic        0.043247      0.566816          0.038336  0.153244  0.424023
afrobeat        0.086579      0.270860          0.253483  0.184596  0.698619
alt-rock        0.055071      0.122162          0.054097  0.210249  0.518260
alternative     0.070101      0.147820          0.038159  0.201376  0.495570
ambient         0.041687      0.776158          0.675362  0.129396  0.168002
{\ldots}                  {\ldots}           {\ldots}               {\ldots}       {\ldots}       {\ldots}
techno          0.064212      0.081414          0.540038  0.159434  0.321878
trance          0.079705      0.035870          0.423501  0.234357  0.276881
trip-hop        0.076303      0.225615          0.383761  0.190342  0.478069
turkish         0.105087      0.321125          0.035603  0.180750  0.462314
world-music     0.041894      0.299980          0.089768  0.250248  0.251048

                  tempo
track\_genre
acoustic     119.010624
afrobeat     119.213337
alt-rock     124.634404
alternative  122.232394
ambient      111.447471
{\ldots}                 {\ldots}
techno       128.255482
trance       133.276726
trip-hop     118.743616
turkish      120.367607
world-music  121.758988

[114 rows x 11 columns]
\end{Verbatim}
\end{tcolorbox}
        
    In order to better visualize these results, we decided to group the
genres even further (as visualizing the differences between 114 grouped
genres may be unclear). To do so, we manually grouped the genres into 10
more over-arching, general genres:

\begin{itemize}
\tightlist
\item
  \textbf{Pop:} cantopop, j-pop, k-pop, mandopop, pop, indie-pop,
  power-pop, pop-film, synth-pop\\
\item
  \textbf{Rock:} alt-rock, alternative, hard-rock, punk-rock,
  psych-rock, rock, rock-n-roll, grunge, emo, rockabilly, guitar\\
\item
  \textbf{Metal:} black-metal, death-metal, heavy-metal, metal,
  metalcore, grindcore\\
\item
  \textbf{Electronic:} edm, electro, electronic, house, garage, techno,
  trance, dubstep, idm, minimal-techno, progressive-house,
  chicago-house, deep-house, detroit-techno, disco, drum-and-bass, dub,
  club, dance, dancehall\\
\item
  \textbf{Hip-Hop:} hip-hop, rap, r-n-b, breakbeat\\
\item
  \textbf{Jazz:} jazz, blues, soul, funk\\
\item
  \textbf{Classical:} classical, opera, piano\\
\item
  \textbf{World:} afrobeat, brazil, british, latin, latino, samba,
  salsa, reggae, reggaeton, tango, world-music, indian, iranian,
  turkish, malay, mpb, pagode, forro, french, german, spanish, swedish\\
\item
  \textbf{Folk:} folk, bluegrass, country, singer-songwriter,
  songwriter, honky-tonk\\
\item
  \textbf{Misc:} acoustic, ambient, anime, children, chill, comedy,
  disney, happy, party, study, sleep, show-tunes, new-age, kids,
  industrial, gospel, goth, groove, hardcore, hardstyle, indie, j-dance,
  j-idol, j-rock, punk, romance, sad, sertanejo, ska, trip-hop
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{77}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{original\PYZus{}data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{spotify.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{genre\PYZus{}groups} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cantopop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{j\PYZhy{}pop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{j\PYZhy{}idol}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k\PYZhy{}pop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mandopop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{indie\PYZhy{}pop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{power\PYZhy{}pop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pop\PYZhy{}film}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{synth\PYZhy{}pop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rock}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{alt\PYZhy{}rock}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{alternative}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hard\PYZhy{}rock}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{indie}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{punk}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{j\PYZhy{}rock}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{punk\PYZhy{}rock}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{psych\PYZhy{}rock}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rock}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rock\PYZhy{}n\PYZhy{}roll}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{grunge}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{emo}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rockabilly}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{guitar}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{metal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black\PYZhy{}metal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{death\PYZhy{}metal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{heavy\PYZhy{}metal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{metal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{metalcore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{grindcore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{electronic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{edm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{electro}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{electronic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{house}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{garage}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{j\PYZhy{}dance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hardcore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hardstyle}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{industrial}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{techno}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{trance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dubstep}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{idm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{minimal\PYZhy{}techno}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{progressive\PYZhy{}house}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{chicago\PYZhy{}house}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{deep\PYZhy{}house}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{detroit\PYZhy{}techno}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{disco}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{drum\PYZhy{}and\PYZhy{}bass}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dub}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{club}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dancehall}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hip\PYZhy{}hop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hip\PYZhy{}hop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rap}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r\PYZhy{}n\PYZhy{}b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{breakbeat}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{trip\PYZhy{}hop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{jazz}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{jazz}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blues}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{soul}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{funk}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ska}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gospel}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{classical}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{classical}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{opera}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{piano}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{world}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{afrobeat}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{brazil}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sertanejo}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{british}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{latin}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{latino}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{samba}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{salsa}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{reggae}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{reggaeton}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tango}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{world\PYZhy{}music}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{indian}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{iranian}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{turkish}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{malay}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mpb}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pagode}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{forro}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{french}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{german}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{spanish}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{swedish}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{folk}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{folk}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bluegrass}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{country}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{singer\PYZhy{}songwriter}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{songwriter}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{honky\PYZhy{}tonk}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{misc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{acoustic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ambient}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{anime}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{children}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{chill}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{comedy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{disney}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{happy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{party}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{study}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sleep}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{show\PYZhy{}tunes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{new\PYZhy{}age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kids}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{goth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{groove}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{romance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sad}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{p}{\PYZcb{}}

\PY{n}{genre\PYZus{}map} \PY{o}{=} \PY{n+nb}{dict}\PY{p}{(}\PY{p}{)}
\PY{k}{for} \PY{n}{genre}\PY{p}{,} \PY{n}{l} \PY{o+ow}{in} \PY{n}{genre\PYZus{}groups}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{k}{for} \PY{n}{original} \PY{o+ow}{in} \PY{n}{l}\PY{p}{:}
        \PY{n}{genre\PYZus{}map}\PY{p}{[}\PY{n}{original}\PY{p}{]} \PY{o}{=} \PY{n}{genre}

\PY{n}{data} \PY{o}{=} \PY{n}{original\PYZus{}data}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{track\PYZus{}genre}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{track\PYZus{}genre}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{map}\PY{p}{(}\PY{n}{genre\PYZus{}map}\PY{p}{)}

\PY{n}{data}\PY{o}{.}\PY{n}{to\PYZus{}csv}\PY{p}{(}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{grouped\PYZus{}cleaned\PYZus{}spotify.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{n}{index}\PY{o}{=}\PY{k+kc}{False}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{78}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{grouped\PYZus{}spotify} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{grouped\PYZus{}cleaned\PYZus{}spotify.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{grouped\PYZus{}spotify}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{track\PYZus{}genre}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{78}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
track\_genre
electronic    24000
world         22984
misc          17855
rock          13996
pop            9999
metal          6000
folk           6000
jazz           5999
hip-hop        4000
classical      2995
Name: count, dtype: int64
\end{Verbatim}
\end{tcolorbox}
        
    Though now we have some imbalance between the different genres, it is
much easier for us to visualize the values. Looking again at the means:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{79}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{temp} \PY{o}{=} \PY{n}{grouped\PYZus{}spotify}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{track\PYZus{}genre}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{n}{by\PYZus{}grouped\PYZus{}genres} \PY{o}{=} \PY{n}{numeric}\PY{p}{(}\PY{n}{grouped\PYZus{}spotify}\PY{p}{)}
\PY{n}{by\PYZus{}grouped\PYZus{}genres}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{track\PYZus{}genre}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{temp}

\PY{n}{grouped\PYZus{}genre\PYZus{}means} \PY{o}{=} \PY{n}{by\PYZus{}grouped\PYZus{}genres}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{track\PYZus{}genre}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
\PY{n}{grouped\PYZus{}genre\PYZus{}means}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{79}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
             popularity    duration\_ms  danceability    energy   loudness  \textbackslash{}
track\_genre
classical     27.671452  223765.185309      0.383762  0.275876 -16.901773
electronic    32.682375  244199.154417      0.628392  0.748102  -6.786162
folk          28.781833  214844.595833      0.557295  0.484704  -9.592910
hip-hop       32.257250  255499.212000      0.657858  0.699074  -7.198840
jazz          28.944157  226728.059177      0.568563  0.578545  -8.072706
metal         30.441833  238321.365167      0.375544  0.893222  -5.626057
misc          32.046766  202753.826211      0.544729  0.519480 -11.052483
pop           41.354135  236116.865087      0.576878  0.647109  -7.206672
rock          36.582809  216122.028079      0.531166  0.673804  -7.515469
world         33.082884  231900.404760      0.600875  0.635310  -7.881580

             speechiness  acousticness  instrumentalness  liveness   valence  \textbackslash{}
track\_genre
classical       0.048077      0.806379          0.432656  0.176999  0.303223
electronic      0.092085      0.122595          0.249558  0.190737  0.432817
folk            0.046643      0.528443          0.048824  0.178637  0.517899
hip-hop         0.095396      0.205441          0.200120  0.216372  0.534306
jazz            0.074014      0.385527          0.028089  0.215736  0.539197
metal           0.098494      0.017465          0.241315  0.251464  0.296137
misc            0.123619      0.488473          0.250852  0.230444  0.411343
pop             0.061161      0.313732          0.028029  0.185500  0.511655
rock            0.062589      0.258902          0.072948  0.204043  0.530961
world           0.082780      0.372472          0.082835  0.243521  0.547976

                  tempo
track\_genre
classical    110.627554
electronic   126.276463
folk         120.840527
hip-hop      123.145587
jazz         119.278760
metal        126.118669
misc         117.992933
pop          124.357350
rock         124.747857
world        120.740060
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{80}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{ranges} \PY{o}{=} \PY{n}{grouped\PYZus{}genre\PYZus{}means}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{grouped\PYZus{}genre\PYZus{}means}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}
\PY{n}{ranges}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{80}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
popularity             13.682683
duration\_ms         52745.385789
danceability            0.282314
energy                  0.617346
loudness               11.275715
speechiness             0.076976
acousticness            0.788914
instrumentalness        0.404627
liveness                0.074465
valence                 0.251839
tempo                  15.648908
dtype: float64
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{81}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{num\PYZus{}vars} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{grouped\PYZus{}genre\PYZus{}means}\PY{o}{.}\PY{n}{columns}\PY{p}{)}
\PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{nrows}\PY{o}{=}\PY{n}{num\PYZus{}vars}\PY{p}{,} \PY{n}{ncols}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{n}{num\PYZus{}vars} \PY{o}{*} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Plot each variable as a barplot}
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{p}{(}\PY{n}{column}\PY{p}{,} \PY{n}{ax}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{grouped\PYZus{}genre\PYZus{}means}\PY{o}{.}\PY{n}{columns}\PY{p}{,} \PY{n}{axes}\PY{p}{)}\PY{p}{)}\PY{p}{:}
    \PY{n}{sorted\PYZus{}data} \PY{o}{=} \PY{n}{grouped\PYZus{}genre\PYZus{}means}\PY{p}{[}\PY{n}{column}\PY{p}{]}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{n}{ascending}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)} \PY{c+c1}{\PYZsh{} sort descending}

    \PY{n}{sns}\PY{o}{.}\PY{n}{barplot}\PY{p}{(}
        \PY{n}{x}\PY{o}{=}\PY{n}{sorted\PYZus{}data}\PY{o}{.}\PY{n}{index}\PY{p}{,}
        \PY{n}{y}\PY{o}{=}\PY{n}{sorted\PYZus{}data}\PY{o}{.}\PY{n}{values}\PY{p}{,}
        \PY{n}{ax}\PY{o}{=}\PY{n}{ax}\PY{p}{,}
    \PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mean }\PY{l+s+si}{\PYZob{}}\PY{n}{column}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ by Genre}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Track Genre}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mean }\PY{l+s+si}{\PYZob{}}\PY{n}{column}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{tick\PYZus{}params}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{rotation}\PY{o}{=}\PY{l+m+mi}{45}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_53_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    From the above visualizations, we can make a few interesting
observations about each variable: - Pop music seems to be the most
popular, though not necessarily by very much (around 5 points difference
from rock). - Duration doesn't seem to have that much variability across
genres when considering the range of the means is about 43000ms, which
is about a 42 second difference. However, hip-hop songs seem to be the
longest in duration compared to other genres. - Classical and metal
tracks seem to score lower on danceability scores compared to other
genres, and hip-hop is the highest for danceability. - Metal scores very
high for energy levels, while classical seems to score lower. -
Similarly, metal scores high for loudness, while classical scores much
lower. In this variable, loudness, the difference between classical and
the rest of the variables is much more apparent. - For the speechiness
variable, folk and classical have the least amount of speechiness, while
miscellaneous genres have the most. - For acousticness, classical music
scores very high while metal scores extremely low. - With
instrumentalness, classical music once again scores high, with pop and
jazz scoring low. - There doesn't seem to be too large of a difference
in liveness across the variables, though metal does score the highest
while classical scores the lowest. - Surprisingly, metal and classical
both score low on valence, with jazz, hip-hop, and world tracks scoring
higher. - Tempo seems to be fairly uniformly distributed across the
genres, with classical and jazz barely scoring lower than the other
genres.

In summary, it seems like classical tracks and metal tracks have the
largest distinction between their variables, and the variables
acousticness and instrumentalness seem to have the most variance across
genres.

    To investigate our genres in our dataset further, such as which features
may play a role in the labelling of genres and if there's a way for us
to predict the genre of a song based on its features, we can prepare our
data for further model and training. We decided we didn't need to do any
feature engineering since several of our variables already ranged from 0
to 1, some with meaningful values (as discussed in the cleaning
section). We also didn't add any extra data through data augmentation
because we already had over 100k samples, which we felt would be
sufficient in our investigation.

In order to prepare our data for more learning models, we split it into
training, validation, and testing datasets (60-20-20). an example of how
we split our data is in the code below, but we re-split it for each
model for the sake of simplicity. We did this both for our original
cleaned dataset and also our dataset grouped by our 10 generalized
genres.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{82}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{random\PYZus{}seed} \PY{o}{=} \PY{l+m+mi}{42}

\PY{c+c1}{\PYZsh{} encode categorical categories}
\PY{k}{for} \PY{n}{column} \PY{o+ow}{in} \PY{n}{categorical\PYZus{}columns}\PY{p}{:}
    \PY{n}{spotify}\PY{p}{[}\PY{n}{column}\PY{p}{]} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Categorical}\PY{p}{(}\PY{n}{spotify}\PY{p}{[}\PY{n}{column}\PY{p}{]}\PY{p}{)}

\PY{c+c1}{\PYZsh{} First split: separate out 20\PYZpc{} for the test set}
\PY{n}{spotify\PYZus{}train\PYZus{}val}\PY{p}{,} \PY{n}{spotify\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{spotify}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}seed}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Second split: separate remaining 80\PYZpc{} into 60\PYZpc{} training and 20\PYZpc{} validation}
\PY{n}{spotify\PYZus{}train}\PY{p}{,} \PY{n}{spotify\PYZus{}val} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{spotify\PYZus{}train\PYZus{}val}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.25}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}seed}\PY{p}{)}  \PY{c+c1}{\PYZsh{} 0.25 * 0.8 = 0.2}

\PY{c+c1}{\PYZsh{} Grouped genre dataset, grouped\PYZus{}spotify}
\PY{c+c1}{\PYZsh{} First split: separate out 20\PYZpc{} for the test set}
\PY{n}{grouped\PYZus{}spotify\PYZus{}train\PYZus{}val}\PY{p}{,} \PY{n}{grouped\PYZus{}spotify\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{grouped\PYZus{}spotify}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}seed}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Second split: separate remaining 80\PYZpc{} into 60\PYZpc{} training and 20\PYZpc{} validation}
\PY{n}{grouped\PYZus{}spotify\PYZus{}train}\PY{p}{,} \PY{n}{grouped\PYZus{}spotify\PYZus{}val} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{grouped\PYZus{}spotify\PYZus{}train\PYZus{}val}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.25}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}seed}\PY{p}{)}  \PY{c+c1}{\PYZsh{} 0.25 * 0.8 = 0.2}
\end{Verbatim}
\end{tcolorbox}

    A summary of the datasets that result from the train-validation-test
splitting is below:

Original 114 genre dataset: - \texttt{spotify\_train} (60\% of the total
dataset) - \texttt{spotify\_val} (20\% of the total dataset) -
\texttt{spotify\_test} (20\% of the total dataset)

Re-grouped 10 genre dataset: - \texttt{grouped\_spotify\_train} (60\% of
the total dataset) - \texttt{grouped\_spotify\_val} (20\% of the total
dataset) - \texttt{grouped\_spotify\_test} (20\% of the total dataset)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{83}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} This code does OHE and removed genre from grouped\PYZus{}spotify}
\PY{n}{response\PYZus{}variable} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{track\PYZus{}genre}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{ohe\PYZus{}column\PYZus{}transformer\PYZus{}wo\PYZus{}genre} \PY{o}{=} \PY{n}{ColumnTransformer}\PY{p}{(}
    \PY{n}{transformers}\PY{o}{=}\PY{p}{[}
        \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ohe}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{OneHotEncoder}\PY{p}{(}\PY{n}{sparse\PYZus{}output}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{,} \PY{p}{[}\PY{n}{col} \PY{k}{for} \PY{n}{col} \PY{o+ow}{in} \PY{n}{categorical\PYZus{}columns} \PY{k}{if} \PY{n}{col} \PY{o}{!=} \PY{n}{response\PYZus{}variable}\PY{p}{]}\PY{p}{)}\PY{p}{,}
        \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{string}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{drop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{string\PYZus{}columns}\PY{p}{)}\PY{p}{,}
        \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{numeric}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{passthrough}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{p}{[}\PY{n}{col} \PY{k}{for} \PY{n}{col} \PY{o+ow}{in} \PY{n}{spotify}\PY{o}{.}\PY{n}{columns} \PY{k}{if} \PY{n}{col} \PY{o}{!=} \PY{n}{response\PYZus{}variable} \PY{o+ow}{and} \PY{n}{col} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{string\PYZus{}columns}\PY{p}{]}\PY{p}{)}
    \PY{p}{]}\PY{p}{,}
    \PY{n}{verbose\PYZus{}feature\PYZus{}names\PYZus{}out}\PY{o}{=}\PY{k+kc}{False}
\PY{p}{)}
\PY{n}{ohe\PYZus{}column\PYZus{}transformer\PYZus{}wo\PYZus{}genre}\PY{o}{.}\PY{n}{set\PYZus{}output}\PY{p}{(}\PY{n}{transform}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pandas}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ohe\PYZus{}column\PYZus{}transformer\PYZus{}wo\PYZus{}genre}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{grouped\PYZus{}spotify}\PY{p}{)}

\PY{n}{ohe\PYZus{}grouped\PYZus{}column\PYZus{}transformer\PYZus{}wo\PYZus{}genre} \PY{o}{=} \PY{n}{ColumnTransformer}\PY{p}{(}
    \PY{n}{transformers}\PY{o}{=}\PY{p}{[}
        \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ohe}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{OneHotEncoder}\PY{p}{(}\PY{n}{sparse\PYZus{}output}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{,} \PY{p}{[}\PY{n}{col} \PY{k}{for} \PY{n}{col} \PY{o+ow}{in} \PY{n}{categorical\PYZus{}columns} \PY{k}{if} \PY{n}{col} \PY{o}{!=} \PY{n}{response\PYZus{}variable}\PY{p}{]}\PY{p}{)}\PY{p}{,}
        \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{string}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{drop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{string\PYZus{}columns}\PY{p}{)}\PY{p}{,}
        \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{numeric}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{passthrough}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{p}{[}\PY{n}{col} \PY{k}{for} \PY{n}{col} \PY{o+ow}{in} \PY{n}{grouped\PYZus{}spotify}\PY{o}{.}\PY{n}{columns} \PY{k}{if} \PY{n}{col} \PY{o}{!=} \PY{n}{response\PYZus{}variable} \PY{o+ow}{and} \PY{n}{col} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{string\PYZus{}columns}\PY{p}{]}\PY{p}{)}
    \PY{p}{]}\PY{p}{,}
    \PY{n}{verbose\PYZus{}feature\PYZus{}names\PYZus{}out}\PY{o}{=}\PY{k+kc}{False}
\PY{p}{)}
\PY{n}{ohe\PYZus{}grouped\PYZus{}column\PYZus{}transformer\PYZus{}wo\PYZus{}genre}\PY{o}{.}\PY{n}{set\PYZus{}output}\PY{p}{(}\PY{n}{transform}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pandas}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ohe\PYZus{}grouped\PYZus{}column\PYZus{}transformer\PYZus{}wo\PYZus{}genre}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{grouped\PYZus{}spotify}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{83}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
ColumnTransformer(transformers=[('ohe', OneHotEncoder(sparse\_output=False),
                                 ['key', 'mode', 'time\_signature']),
                                ('string', 'drop',
                                 ['track\_id', 'artists', 'album\_name',
                                  'track\_name']),
                                ('numeric', 'passthrough',
                                 ['popularity', 'duration\_ms', 'explicit',
                                  'danceability', 'energy', 'key', 'loudness',
                                  'mode', 'speechiness', 'acousticness',
                                  'instrumentalness', 'liveness', 'valence',
                                  'tempo', 'time\_signature'])],
                  verbose\_feature\_names\_out=False)
\end{Verbatim}
\end{tcolorbox}
        
    \subsection{Linear + Polynomial
Regression}\label{linear-polynomial-regression}

For our linear regression, we chose to use the \texttt{energy} variable
as our response variable, as it seemed to have the most relationships
with other variables based on our correlation heatmap during our EDA,
and we were interested in finding out what may contribute to the energy
of a song.

To find out our predictor variables, we used forward feature selection
to see which predictors may be most correlated with our response
variable (energy).

    To find out our predictor variables, we used forward feature selection
to see which predictors may be most correlated with our response
variable (energy).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{52}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{response} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{energy}\PY{l+s+s2}{\PYZdq{}} \PY{c+c1}{\PYZsh{} this was chosen based on correlation heatmap}

\PY{c+c1}{\PYZsh{} Making sure only numeric variables are used:}
\PY{n}{num\PYZus{}spotify\PYZus{}train} \PY{o}{=} \PY{n}{numeric}\PY{p}{(}\PY{n}{spotify\PYZus{}train}\PY{p}{)}
\PY{n}{num\PYZus{}spotify\PYZus{}val} \PY{o}{=} \PY{n}{numeric}\PY{p}{(}\PY{n}{spotify\PYZus{}val}\PY{p}{)}
\PY{n}{num\PYZus{}spotify\PYZus{}test} \PY{o}{=} \PY{n}{numeric}\PY{p}{(}\PY{n}{spotify\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Reshape the data to fit the model}
\PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{num\PYZus{}spotify\PYZus{}train}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{n}{response}\PY{p}{)}
\PY{n}{y} \PY{o}{=} \PY{n}{num\PYZus{}spotify\PYZus{}train}\PY{p}{[}\PY{n}{response}\PY{p}{]}

\PY{n}{lin\PYZus{}reg} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Select features}
\PY{n}{selector} \PY{o}{=} \PY{n}{SequentialFeatureSelector}\PY{p}{(}
    \PY{n}{lin\PYZus{}reg}\PY{p}{,}
    \PY{n}{n\PYZus{}features\PYZus{}to\PYZus{}select}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{n}{direction}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{forward}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{n}{cv} \PY{o}{=} \PY{l+m+mi}{5}
\PY{p}{)}

\PY{n}{selector}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y}\PY{p}{)}
\PY{n}{selected\PYZus{}features} \PY{o}{=} \PY{n}{selector}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names\PYZus{}out}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Selected Features: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{selected\PYZus{}features}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Selected Features:  ['loudness' 'acousticness' 'instrumentalness' 'liveness'
'valence']
    \end{Verbatim}

    Based on this we found that
\texttt{\textquotesingle{}loudness\textquotesingle{},\ \textquotesingle{}acousticness\textquotesingle{},\ \textquotesingle{}instrumentalness\textquotesingle{},\ \textquotesingle{}liveness\textquotesingle{},}
and \texttt{\textquotesingle{}valence\textquotesingle{}} were our
selected features, and graphed these with energy as the response
variable to investigate the relationships.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{53}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X} \PY{o}{=} \PY{n}{selector}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}

\PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{selector}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{num\PYZus{}spotify\PYZus{}test}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{n}{response}\PY{p}{)}\PY{p}{)}
\PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{num\PYZus{}spotify\PYZus{}test}\PY{p}{[}\PY{n}{response}\PY{p}{]}

\PY{n}{X\PYZus{}val} \PY{o}{=} \PY{n}{selector}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{num\PYZus{}spotify\PYZus{}val}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{n}{response}\PY{p}{)}\PY{p}{)}
\PY{n}{y\PYZus{}val} \PY{o}{=} \PY{n}{num\PYZus{}spotify\PYZus{}val}\PY{p}{[}\PY{n}{response}\PY{p}{]}

\PY{n}{lin\PYZus{}reg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}
\PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{lin\PYZus{}reg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Plotting scatter plot with a regression line}
\PY{k}{for} \PY{n}{feature} \PY{o+ow}{in} \PY{n}{selected\PYZus{}features}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Scatter plot of the data points}
    \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{feature}\PY{p}{]}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Data Points}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Calculate the regression line}
    \PY{n}{feature\PYZus{}values} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{feature}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{temp\PYZus{}model} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}
    \PY{n}{temp\PYZus{}model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{feature\PYZus{}values}\PY{p}{,} \PY{n}{y}\PY{p}{)}
    \PY{n}{y\PYZus{}line} \PY{o}{=} \PY{n}{temp\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{feature\PYZus{}values}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Plot the regression line}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{feature\PYZus{}values}\PY{p}{,} \PY{n}{y\PYZus{}line}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Regression Line}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{n}{feature}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{n}{response}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}

    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_63_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_63_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_63_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_63_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_63_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Based on these graphs, some of our correlations are likely not linear,
and others likely have a large amount of variance. We calculated the
evaluation metrics for our linear regression model to see how it was
performing.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{54}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Calculating Evaluation Metrics:}
\PY{n}{y\PYZus{}val\PYZus{}pred} \PY{o}{=} \PY{n}{lin\PYZus{}reg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)} \PY{c+c1}{\PYZsh{} Predict on validation set}

\PY{c+c1}{\PYZsh{} Calculate metrics for the training set}
\PY{n}{train\PYZus{}mse} \PY{o}{=} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)} \PY{c+c1}{\PYZsh{} Mean squared error}
\PY{n}{train\PYZus{}rmse} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{train\PYZus{}mse}\PY{p}{)} \PY{c+c1}{\PYZsh{} Root mean squared error}
\PY{n}{train\PYZus{}mae} \PY{o}{=} \PY{n}{mean\PYZus{}absolute\PYZus{}error}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)} \PY{c+c1}{\PYZsh{} Mean average error}
\PY{n}{train\PYZus{}mad} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{y} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Mean absolute deviation}
\PY{n}{train\PYZus{}r2} \PY{o}{=} \PY{n}{r2\PYZus{}score}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)} \PY{c+c1}{\PYZsh{} R\PYZca{}2 (coefficient of determination)}

\PY{c+c1}{\PYZsh{} Calculate metrics for the validation set}
\PY{n}{val\PYZus{}mse} \PY{o}{=} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val\PYZus{}pred}\PY{p}{)}
\PY{n}{val\PYZus{}rmse} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{val\PYZus{}mse}\PY{p}{)}
\PY{n}{val\PYZus{}mae} \PY{o}{=} \PY{n}{mean\PYZus{}absolute\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val\PYZus{}pred}\PY{p}{)}
\PY{n}{val\PYZus{}mad} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{y\PYZus{}val} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}val\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\PY{n}{val\PYZus{}r2} \PY{o}{=} \PY{n}{r2\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val\PYZus{}pred}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Print Results:}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training MSE: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}mse}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, rMSE: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}rmse}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, MAE: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}mae}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, MAD: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}mad}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, R²: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}r2}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Validation MSE: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}mse}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, rMSE: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}rmse}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, MAE: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}mae}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, MAD: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}mad}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, R²: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}r2}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} bias variance}
\PY{n}{avg\PYZus{}expected\PYZus{}loss}\PY{p}{,} \PY{n}{avg\PYZus{}bias}\PY{p}{,} \PY{n}{avg\PYZus{}var} \PY{o}{=} \PY{n}{bias\PYZus{}variance\PYZus{}decomp}\PY{p}{(}
    \PY{n}{lin\PYZus{}reg}\PY{p}{,}
    \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{o}{.}\PY{n}{values}\PY{p}{,}
    \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{values}\PY{p}{,}
    \PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mse}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{n}{random\PYZus{}seed}\PY{o}{=}\PY{l+m+mi}{123}
\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Loss, variance, and bias: }\PY{l+s+si}{\PYZob{}}\PY{n}{avg\PYZus{}expected\PYZus{}loss}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, }\PY{l+s+si}{\PYZob{}}\PY{n}{avg\PYZus{}bias}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, }\PY{l+s+si}{\PYZob{}}\PY{n}{avg\PYZus{}var}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Training MSE: 0.01591975141780648, rMSE: 0.12617349728768906, MAE:
0.0986015100486982, MAD: 0.0986015100486982, R²: 0.7477655716969742
Validation MSE: 0.016297699800804963, rMSE: 0.1276624447549277, MAE:
0.09952336547066755, MAD: 0.09952336547066755, R²: 0.7385932898496084
Loss, variance, and bias: 0.01592652204991807, 0.015924634631379178,
1.8874185388953818e-06
    \end{Verbatim}

    Based on the evaluation metrics we calculated above, our model seems to
fit the data decently. Most notably, the MSE is very low and close
together for the training and validation sets, 0.016 for both. This
suggests there is low error and the model fits the data.

The R\^{}2 values (0.743 and 0.746) are moderately strong, which
indicates that the model is explaining a good amount of the variance in
the dependent variable (energy). However, we know based on the graphs
(that do not look very linear) that we could probably improve our model
if we were to use a different model, such as a polynomial regression.
However, since the exploration on energy doesn't necessarily contribute
to our project goal, we decided to switch directions and look further
into genre instead.

Though linear/polynomial regression isn't necessarily a model that
aligns with our project goals (which focuses on investigating and
predicting the genres, a categorical variable, or different songs), and
thus we chose not to include it in our main report, we still did some
light explorations of the relationships between our different variables
for different genres using regression.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{55}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Make a dictionary for the data for efficiency}
\PY{n}{genre\PYZus{}groups} \PY{o}{=} \PY{p}{\PYZob{}}\PY{n}{genre}\PY{p}{:} \PY{n}{group} \PY{k}{for} \PY{n}{genre}\PY{p}{,} \PY{n}{group} \PY{o+ow}{in} \PY{n}{spotify}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{track\PYZus{}genre}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{\PYZcb{}}
\PY{n}{grouped\PYZus{}genre\PYZus{}groups} \PY{o}{=} \PY{p}{\PYZob{}}\PY{n}{genre}\PY{p}{:} \PY{n}{group} \PY{k}{for} \PY{n}{genre}\PY{p}{,} \PY{n}{group} \PY{o+ow}{in} \PY{n}{grouped\PYZus{}spotify}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{track\PYZus{}genre}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{\PYZcb{}}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{spotify}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{track\PYZus{}genre}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{grouped\PYZus{}spotify}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{track\PYZus{}genre}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
['acoustic' 'afrobeat' 'alt-rock' 'alternative' 'ambient' 'anime'
 'black-metal' 'bluegrass' 'blues' 'brazil' 'breakbeat' 'british'
 'cantopop' 'chicago-house' 'children' 'chill' 'classical' 'club' 'comedy'
 'country' 'dance' 'dancehall' 'death-metal' 'deep-house' 'detroit-techno'
 'disco' 'disney' 'drum-and-bass' 'dub' 'dubstep' 'edm' 'electro'
 'electronic' 'emo' 'folk' 'forro' 'french' 'funk' 'garage' 'german'
 'gospel' 'goth' 'grindcore' 'groove' 'grunge' 'guitar' 'happy'
 'hard-rock' 'hardcore' 'hardstyle' 'heavy-metal' 'hip-hop' 'honky-tonk'
 'house' 'idm' 'indian' 'indie' 'indie-pop' 'industrial' 'iranian'
 'j-dance' 'j-idol' 'j-pop' 'j-rock' 'jazz' 'k-pop' 'kids' 'latin'
 'latino' 'malay' 'mandopop' 'metal' 'metalcore' 'minimal-techno' 'mpb'
 'new-age' 'opera' 'pagode' 'party' 'piano' 'pop' 'pop-film' 'power-pop'
 'progressive-house' 'psych-rock' 'punk' 'punk-rock' 'r-n-b' 'reggae'
 'reggaeton' 'rock' 'rock-n-roll' 'rockabilly' 'romance' 'sad' 'salsa'
 'samba' 'sertanejo' 'show-tunes' 'singer-songwriter' 'ska' 'sleep'
 'songwriter' 'soul' 'spanish' 'study' 'swedish' 'synth-pop' 'tango'
 'techno' 'trance' 'trip-hop' 'turkish' 'world-music']
['classical' 'electronic' 'folk' 'hip-hop' 'jazz' 'metal' 'misc' 'pop'
 'rock' 'world']
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{56}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Selecting only the numeric features}
\PY{n}{numeric\PYZus{}spotify} \PY{o}{=} \PY{n}{spotify}\PY{o}{.}\PY{n}{select\PYZus{}dtypes}\PY{p}{(}\PY{n}{include}\PY{o}{=}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{number}\PY{p}{]}\PY{p}{)}
\PY{n}{numeric\PYZus{}spotify} \PY{o}{=} \PY{n}{numeric\PYZus{}spotify}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{o}{\PYZti{}}\PY{n}{numeric\PYZus{}spotify}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{isin}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mode}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{key}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{time\PYZus{}signature}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{]}
\PY{n}{numeric\PYZus{}spotify}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{56}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
        popularity  duration\_ms  danceability  energy  loudness  speechiness  \textbackslash{}
0               73       230666         0.676  0.4610    -6.746       0.1430
1               55       149610         0.420  0.1660   -17.235       0.0763
2               57       210826         0.438  0.3590    -9.734       0.0557
3               71       201933         0.266  0.0596   -18.515       0.0363
4               82       198853         0.618  0.4430    -9.681       0.0526
{\ldots}            {\ldots}          {\ldots}           {\ldots}     {\ldots}       {\ldots}          {\ldots}
113823          21       384999         0.172  0.2350   -16.393       0.0422
113824          22       385000         0.174  0.1170   -18.318       0.0401
113825          22       271466         0.629  0.3290   -10.895       0.0420
113826          41       283893         0.587  0.5060   -10.889       0.0297
113827          22       241826         0.526  0.4870   -10.204       0.0725

        acousticness  instrumentalness  liveness  valence    tempo
0             0.0322          0.000001    0.3580   0.7150   87.917
1             0.9240          0.000006    0.1010   0.2670   77.489
2             0.2100          0.000000    0.1170   0.1200   76.332
3             0.9050          0.000071    0.1320   0.1430  181.740
4             0.4690          0.000000    0.0829   0.1670  119.949
{\ldots}              {\ldots}               {\ldots}       {\ldots}      {\ldots}      {\ldots}
113823        0.6400          0.928000    0.0863   0.0339  125.995
113824        0.9940          0.976000    0.1050   0.0350   85.239
113825        0.8670          0.000000    0.0839   0.7430  132.378
113826        0.3810          0.000000    0.2700   0.4130  135.960
113827        0.6810          0.000000    0.0893   0.7080   79.198

[113828 rows x 11 columns]
\end{Verbatim}
\end{tcolorbox}
        
    We'll start with using linear regression. We looked at the linear
regression between each of the 114 genres and each other variable to see
if there seemed to be any strong relationships. We investigated the
R\^{}2, variance, and bias for the fits.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{57}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{numeric\PYZus{}dict} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{popularity}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{duration\PYZus{}ms}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{danceability}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{energy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loudness}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{speechiness}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{acousticness}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{instrumentalness}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{liveness}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valence}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tempo}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{0}
\PY{p}{\PYZcb{}}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Modeling linear regression of genres to all other variables}

\PY{n}{highest\PYZus{}r2} \PY{o}{=} \PY{l+m+mi}{0}
\PY{n}{highest\PYZus{}r2\PYZus{}genre} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{highest\PYZus{}r2\PYZus{}response} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}

\PY{k}{for} \PY{n}{genre} \PY{o+ow}{in} \PY{n}{genre\PYZus{}groups}\PY{p}{:}
    \PY{n}{df} \PY{o}{=} \PY{n}{genre\PYZus{}groups}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{n}{genre}\PY{p}{,} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{n}{numeric\PYZus{}df} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{select\PYZus{}dtypes}\PY{p}{(}\PY{n}{include}\PY{o}{=}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{number}\PY{p}{]}\PY{p}{)}
    \PY{n}{numeric\PYZus{}df} \PY{o}{=} \PY{n}{numeric\PYZus{}df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{o}{\PYZti{}}\PY{n}{numeric\PYZus{}df}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{isin}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mode}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{key}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{time\PYZus{}signature}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{]}

    \PY{n}{linear\PYZus{}reg} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Select features}
    \PY{n}{selector} \PY{o}{=} \PY{n}{SequentialFeatureSelector}\PY{p}{(}
            \PY{n}{linear\PYZus{}reg}\PY{p}{,}
            \PY{n}{n\PYZus{}features\PYZus{}to\PYZus{}select}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
            \PY{n}{direction}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{forward}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
            \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
            \PY{n}{cv} \PY{o}{=} \PY{l+m+mi}{5}
        \PY{p}{)}

    \PY{n}{max\PYZus{}train\PYZus{}r2} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{max\PYZus{}validation\PYZus{}r2} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{max\PYZus{}response} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}

    \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{numeric\PYZus{}df}\PY{o}{.}\PY{n}{columns}\PY{p}{:}
        \PY{n}{random\PYZus{}seed} \PY{o}{=} \PY{l+m+mi}{42}
        \PY{n}{response} \PY{o}{=} \PY{n}{x}
        \PY{c+c1}{\PYZsh{} Splitting the data}
        \PY{c+c1}{\PYZsh{} First split: separate out 20\PYZpc{} for the test set}
        \PY{n}{train\PYZus{}val}\PY{p}{,} \PY{n}{test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{numeric\PYZus{}df}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}seed}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Second split: separate remaining 80\PYZpc{} into 60\PYZpc{} training and 40\PYZpc{} validation}
        \PY{n}{train}\PY{p}{,} \PY{n}{val} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{train\PYZus{}val}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.25}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}seed}\PY{p}{)}  \PY{c+c1}{\PYZsh{} 0.25 * 0.8 = 0.2}

        \PY{c+c1}{\PYZsh{} Reshape the data to fit the model}
        \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{train}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{n}{response}\PY{p}{)}
        \PY{n}{y} \PY{o}{=} \PY{n}{train}\PY{p}{[}\PY{n}{response}\PY{p}{]}


        \PY{n}{selector}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y}\PY{p}{)}
        \PY{n}{selected\PYZus{}features} \PY{o}{=} \PY{n}{selector}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names\PYZus{}out}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Transform data sets}
        \PY{n}{X} \PY{o}{=} \PY{n}{selector}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}

        \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{selector}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{test}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{n}{response}\PY{p}{)}\PY{p}{)}
        \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{test}\PY{p}{[}\PY{n}{response}\PY{p}{]}

        \PY{n}{X\PYZus{}val} \PY{o}{=} \PY{n}{selector}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{val}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{n}{response}\PY{p}{)}\PY{p}{)}
        \PY{n}{y\PYZus{}val} \PY{o}{=} \PY{n}{val}\PY{p}{[}\PY{n}{response}\PY{p}{]}

        \PY{n}{linear\PYZus{}reg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}
        \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{linear\PYZus{}reg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Calculating Evaluation Metrics:}
        \PY{n}{y\PYZus{}val\PYZus{}pred} \PY{o}{=} \PY{n}{linear\PYZus{}reg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Calculate metrics for the training set}
        \PY{n}{train\PYZus{}mse} \PY{o}{=} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)} \PY{c+c1}{\PYZsh{} Mean squared error}
        \PY{n}{train\PYZus{}rmse} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{train\PYZus{}mse}\PY{p}{)} \PY{c+c1}{\PYZsh{} Root mean squared error}
        \PY{n}{train\PYZus{}mae} \PY{o}{=} \PY{n}{mean\PYZus{}absolute\PYZus{}error}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)} \PY{c+c1}{\PYZsh{} Mean average error}
        \PY{n}{train\PYZus{}mad} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{y} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Mean absolute deviation}
        \PY{n}{train\PYZus{}r2} \PY{o}{=} \PY{n}{r2\PYZus{}score}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)} \PY{c+c1}{\PYZsh{} R\PYZca{}2 (coefficient of determination)}

        \PY{c+c1}{\PYZsh{} Calculate metrics for the validation set}
        \PY{n}{val\PYZus{}mse} \PY{o}{=} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val\PYZus{}pred}\PY{p}{)}
        \PY{n}{val\PYZus{}rmse} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{val\PYZus{}mse}\PY{p}{)}
        \PY{n}{val\PYZus{}mae} \PY{o}{=} \PY{n}{mean\PYZus{}absolute\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val\PYZus{}pred}\PY{p}{)}
        \PY{n}{val\PYZus{}mad} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{y\PYZus{}val} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}val\PYZus{}pred}\PY{p}{)}\PY{p}{)}
        \PY{n}{val\PYZus{}r2} \PY{o}{=} \PY{n}{r2\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val\PYZus{}pred}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Print Results:}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{genre}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ x }\PY{l+s+si}{\PYZob{}}\PY{n}{response}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ \PYZhy{} Training MSE: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}mse}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, rMSE: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}rmse}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, MAE: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}mae}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, MAD: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}mad}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, R²: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}r2}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{genre}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ x }\PY{l+s+si}{\PYZob{}}\PY{n}{response}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ \PYZhy{} Validation MSE: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}mse}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, rMSE: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}rmse}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, MAE: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}mae}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, MAD: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}mad}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, R²: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}r2}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

        \PY{k}{if} \PY{n}{val\PYZus{}r2} \PY{o}{\PYZgt{}} \PY{n}{max\PYZus{}validation\PYZus{}r2}\PY{p}{:}
            \PY{n}{max\PYZus{}train\PYZus{}r2} \PY{o}{=} \PY{n}{train\PYZus{}r2}
            \PY{n}{max\PYZus{}validation\PYZus{}r2} \PY{o}{=} \PY{n}{val\PYZus{}r2}
            \PY{n}{max\PYZus{}response} \PY{o}{=} \PY{n}{x}

        \PY{c+c1}{\PYZsh{} bias variance}
        \PY{n}{avg\PYZus{}expected\PYZus{}loss}\PY{p}{,} \PY{n}{avg\PYZus{}bias}\PY{p}{,} \PY{n}{avg\PYZus{}var} \PY{o}{=} \PY{n}{bias\PYZus{}variance\PYZus{}decomp}\PY{p}{(}
            \PY{n}{linear\PYZus{}reg}\PY{p}{,}
            \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{o}{.}\PY{n}{values}\PY{p}{,}
            \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{values}\PY{p}{,}
            \PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mse}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
            \PY{n}{random\PYZus{}seed}\PY{o}{=}\PY{l+m+mi}{123}
        \PY{p}{)}

        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{genre}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ x }\PY{l+s+si}{\PYZob{}}\PY{n}{response}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ \PYZhy{} Loss, Variance, and Bias: }\PY{l+s+si}{\PYZob{}}\PY{n}{avg\PYZus{}expected\PYZus{}loss}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, }\PY{l+s+si}{\PYZob{}}\PY{n}{avg\PYZus{}bias}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, }\PY{l+s+si}{\PYZob{}}\PY{n}{avg\PYZus{}var}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Response Variable with the Highest Validation R\PYZca{}2 for }\PY{l+s+si}{\PYZob{}}\PY{n}{genre}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{: }\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}response}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Train R\PYZca{}2 for }\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}response}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ in }\PY{l+s+si}{\PYZob{}}\PY{n}{genre}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{: }\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}train\PYZus{}r2}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Validation R\PYZca{}2 for }\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}response}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ in }\PY{l+s+si}{\PYZob{}}\PY{n}{genre}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{: }\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}validation\PYZus{}r2}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{numeric\PYZus{}dict}\PY{p}{[}\PY{n}{max\PYZus{}response}\PY{p}{]}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Storing the variable / genre with the highest R\PYZca{}2 values}
    \PY{k}{if}\PY{p}{(}\PY{n}{max\PYZus{}validation\PYZus{}r2} \PY{o}{\PYZgt{}} \PY{n}{highest\PYZus{}r2}\PY{p}{)}\PY{p}{:}
        \PY{n}{highest\PYZus{}r2} \PY{o}{=} \PY{n}{max\PYZus{}validation\PYZus{}r2}
        \PY{n}{highest\PYZus{}r2\PYZus{}genre} \PY{o}{=} \PY{n}{genre}
        \PY{n}{highest\PYZus{}r2\PYZus{}response} \PY{o}{=} \PY{n}{response}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{numeric\PYZus{}dict}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The highest Validation R\PYZca{}2 was between }\PY{l+s+si}{\PYZob{}}\PY{n}{genre}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ and }\PY{l+s+si}{\PYZob{}}\PY{n}{response}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, with a score of }\PY{l+s+si}{\PYZob{}}\PY{n}{highest\PYZus{}r2}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Output:
\texttt{The\ highest\ Validation\ R\^{}2\ was\ between\ world-music\ and\ tempo,\ with\ a\ score\ of\ 0.9132660013292169.}

    Based on our linear regression investigation, it seems like energy
tended to have the highest validation correlation with various genres,
with 101 genres that were correlated the most with energy compared to
other features. Following energy, 11 genres were more correlated with
loudness, and 1 genre for both speechiness and valence had highest
correlations. The relationship that seemed to have the strongest linear
correlation was between the world-music genre's variables, with the
tempo variable as the response.

    We decided to also try a quadratic regression on our data to see if the
increase in degree may have any significant effects.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{59}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Define the degree of the polynomial, in this case, quadratic}
\PY{n}{degree} \PY{o}{=} \PY{l+m+mi}{2}

\PY{c+c1}{\PYZsh{} Create a pipeline for Polynomial Regression}
\PY{n}{poly\PYZus{}reg} \PY{o}{=} \PY{n}{make\PYZus{}pipeline}\PY{p}{(}
    \PY{n}{PolynomialFeatures}\PY{p}{(}\PY{n}{degree}\PY{o}{=}\PY{n}{degree}\PY{p}{)}\PY{p}{,}
    \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Modeling quadratic regression (degree = 2) of genres to all other variables}

\PY{n}{numeric\PYZus{}dict2} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{popularity}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{duration\PYZus{}ms}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{danceability}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{energy}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loudness}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{speechiness}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{acousticness}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{instrumentalness}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{liveness}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valence}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tempo}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mi}{0}
\PY{p}{\PYZcb{}}

\PY{n}{highest\PYZus{}r2} \PY{o}{=} \PY{l+m+mi}{0}
\PY{n}{highest\PYZus{}r2\PYZus{}genre} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{highest\PYZus{}r2\PYZus{}response} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}

\PY{k}{for} \PY{n}{genre} \PY{o+ow}{in} \PY{n}{genre\PYZus{}groups}\PY{p}{:}
    \PY{n}{df} \PY{o}{=} \PY{n}{genre\PYZus{}groups}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{n}{genre}\PY{p}{,} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{n}{numeric\PYZus{}df} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{select\PYZus{}dtypes}\PY{p}{(}\PY{n}{include}\PY{o}{=}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{number}\PY{p}{]}\PY{p}{)}
    \PY{n}{numeric\PYZus{}df} \PY{o}{=} \PY{n}{numeric\PYZus{}df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{o}{\PYZti{}}\PY{n}{numeric\PYZus{}df}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{isin}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mode}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{key}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{time\PYZus{}signature}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{]}

    \PY{c+c1}{\PYZsh{} Select features}
    \PY{n}{selector} \PY{o}{=} \PY{n}{SequentialFeatureSelector}\PY{p}{(}
            \PY{n}{linear\PYZus{}reg}\PY{p}{,}
            \PY{n}{n\PYZus{}features\PYZus{}to\PYZus{}select}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
            \PY{n}{direction}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{forward}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
            \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
            \PY{n}{cv} \PY{o}{=} \PY{l+m+mi}{5}
        \PY{p}{)}

    \PY{n}{max\PYZus{}train\PYZus{}r2} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{max\PYZus{}validation\PYZus{}r2} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{max\PYZus{}response} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}

    \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{numeric\PYZus{}df}\PY{o}{.}\PY{n}{columns}\PY{p}{:}
        \PY{n}{random\PYZus{}seed} \PY{o}{=} \PY{l+m+mi}{42}
        \PY{n}{response} \PY{o}{=} \PY{n}{x}
        \PY{c+c1}{\PYZsh{} Splitting the data}
        \PY{c+c1}{\PYZsh{} First split: separate out 20\PYZpc{} for the test set}
        \PY{n}{train\PYZus{}val}\PY{p}{,} \PY{n}{test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{numeric\PYZus{}df}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}seed}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Second split: separate remaining 80\PYZpc{} into 60\PYZpc{} training and 40\PYZpc{} validation}
        \PY{n}{train}\PY{p}{,} \PY{n}{val} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{train\PYZus{}val}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.25}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}seed}\PY{p}{)}  \PY{c+c1}{\PYZsh{} 0.25 * 0.8 = 0.2}

        \PY{c+c1}{\PYZsh{} Reshape the data to fit the model}
        \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{train}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{n}{response}\PY{p}{)}
        \PY{n}{y} \PY{o}{=} \PY{n}{train}\PY{p}{[}\PY{n}{response}\PY{p}{]}


        \PY{n}{selector}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y}\PY{p}{)}
        \PY{n}{selected\PYZus{}features} \PY{o}{=} \PY{n}{selector}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names\PYZus{}out}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Transform data sets}
        \PY{n}{X} \PY{o}{=} \PY{n}{selector}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}

        \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{selector}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{test}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{n}{response}\PY{p}{)}\PY{p}{)}
        \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{test}\PY{p}{[}\PY{n}{response}\PY{p}{]}

        \PY{n}{X\PYZus{}val} \PY{o}{=} \PY{n}{selector}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{val}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{n}{response}\PY{p}{)}\PY{p}{)}
        \PY{n}{y\PYZus{}val} \PY{o}{=} \PY{n}{val}\PY{p}{[}\PY{n}{response}\PY{p}{]}

        \PY{c+c1}{\PYZsh{} Fit the Polynomial Regression model}
        \PY{n}{poly\PYZus{}reg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}
        \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{poly\PYZus{}reg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Calculating Evaluation Metrics:}
        \PY{n}{y\PYZus{}val\PYZus{}pred} \PY{o}{=} \PY{n}{poly\PYZus{}reg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Calculate metrics for the training set}
        \PY{n}{train\PYZus{}mse} \PY{o}{=} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)} \PY{c+c1}{\PYZsh{} Mean squared error}
        \PY{n}{train\PYZus{}rmse} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{train\PYZus{}mse}\PY{p}{)} \PY{c+c1}{\PYZsh{} Root mean squared error}
        \PY{n}{train\PYZus{}mae} \PY{o}{=} \PY{n}{mean\PYZus{}absolute\PYZus{}error}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)} \PY{c+c1}{\PYZsh{} Mean average error}
        \PY{n}{train\PYZus{}mad} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{y} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Mean absolute deviation}
        \PY{n}{train\PYZus{}r2} \PY{o}{=} \PY{n}{r2\PYZus{}score}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)} \PY{c+c1}{\PYZsh{} R\PYZca{}2 (coefficient of determination)}

        \PY{c+c1}{\PYZsh{} Calculate metrics for the validation set}
        \PY{n}{val\PYZus{}mse} \PY{o}{=} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val\PYZus{}pred}\PY{p}{)}
        \PY{n}{val\PYZus{}rmse} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{val\PYZus{}mse}\PY{p}{)}
        \PY{n}{val\PYZus{}mae} \PY{o}{=} \PY{n}{mean\PYZus{}absolute\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val\PYZus{}pred}\PY{p}{)}
        \PY{n}{val\PYZus{}mad} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{y\PYZus{}val} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}val\PYZus{}pred}\PY{p}{)}\PY{p}{)}
        \PY{n}{val\PYZus{}r2} \PY{o}{=} \PY{n}{r2\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val\PYZus{}pred}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Print Results:}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{genre}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ x }\PY{l+s+si}{\PYZob{}}\PY{n}{response}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ \PYZhy{} Training MSE: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}mse}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, rMSE: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}rmse}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, MAE: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}mae}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, MAD: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}mad}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, R²: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}r2}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{genre}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ x }\PY{l+s+si}{\PYZob{}}\PY{n}{response}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ \PYZhy{} Validation MSE: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}mse}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, rMSE: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}rmse}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, MAE: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}mae}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, MAD: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}mad}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, R²: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}r2}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

        \PY{k}{if} \PY{n}{val\PYZus{}r2} \PY{o}{\PYZgt{}} \PY{n}{max\PYZus{}validation\PYZus{}r2}\PY{p}{:}
            \PY{n}{max\PYZus{}train\PYZus{}r2} \PY{o}{=} \PY{n}{train\PYZus{}r2}
            \PY{n}{max\PYZus{}validation\PYZus{}r2} \PY{o}{=} \PY{n}{val\PYZus{}r2}
            \PY{n}{max\PYZus{}response} \PY{o}{=} \PY{n}{x}

        \PY{c+c1}{\PYZsh{} bias variance}
        \PY{n}{avg\PYZus{}expected\PYZus{}loss}\PY{p}{,} \PY{n}{avg\PYZus{}bias}\PY{p}{,} \PY{n}{avg\PYZus{}var} \PY{o}{=} \PY{n}{bias\PYZus{}variance\PYZus{}decomp}\PY{p}{(}
            \PY{n}{linear\PYZus{}reg}\PY{p}{,}
            \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{o}{.}\PY{n}{values}\PY{p}{,}
            \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{values}\PY{p}{,}
            \PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mse}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
            \PY{n}{random\PYZus{}seed}\PY{o}{=}\PY{l+m+mi}{123}
        \PY{p}{)}

        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{genre}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ x }\PY{l+s+si}{\PYZob{}}\PY{n}{response}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ \PYZhy{} Loss, Variance, and Bias: }\PY{l+s+si}{\PYZob{}}\PY{n}{avg\PYZus{}expected\PYZus{}loss}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, }\PY{l+s+si}{\PYZob{}}\PY{n}{avg\PYZus{}bias}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, }\PY{l+s+si}{\PYZob{}}\PY{n}{avg\PYZus{}var}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Response Variable with the Highest Validation R\PYZca{}2 for }\PY{l+s+si}{\PYZob{}}\PY{n}{genre}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{: }\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}response}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Train R\PYZca{}2 for }\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}response}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ in }\PY{l+s+si}{\PYZob{}}\PY{n}{genre}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{: }\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}train\PYZus{}r2}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Validation R\PYZca{}2 for }\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}response}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ in }\PY{l+s+si}{\PYZob{}}\PY{n}{genre}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{: }\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}validation\PYZus{}r2}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{numeric\PYZus{}dict2}\PY{p}{[}\PY{n}{max\PYZus{}response}\PY{p}{]}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Storing the variable / genre with the highest R\PYZca{}2 values}
    \PY{k}{if}\PY{p}{(}\PY{n}{max\PYZus{}validation\PYZus{}r2} \PY{o}{\PYZgt{}} \PY{n}{highest\PYZus{}r2}\PY{p}{)}\PY{p}{:}
        \PY{n}{highest\PYZus{}r2} \PY{o}{=} \PY{n}{max\PYZus{}validation\PYZus{}r2}
        \PY{n}{highest\PYZus{}r2\PYZus{}genre} \PY{o}{=} \PY{n}{genre}
        \PY{n}{highest\PYZus{}r2\PYZus{}response} \PY{o}{=} \PY{n}{response}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{numeric\PYZus{}dict}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The highest Validation R\PYZca{}2 was between }\PY{l+s+si}{\PYZob{}}\PY{n}{genre}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ and }\PY{l+s+si}{\PYZob{}}\PY{n}{response}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, with a score of }\PY{l+s+si}{\PYZob{}}\PY{n}{highest\PYZus{}r2}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Output:
\texttt{The\ highest\ Validation\ R\^{}2\ was\ between\ world-music\ and\ tempo,\ with\ a\ score\ of\ 0.9293826016602357.}

    Compared to the linear regression earlier, the quadratic regression
didn't seem to have much of a change at all, other than the highest
validation R\^{}2 value between the world-music genre's variables and
tempo slightly increasing from 0.913 to 0.929.

We also tried linear regression with our 10 manually grouped genres
rather than the original 114 to see if it would make a difference.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Modeling linear regression of 10 generalized genres to all other variables}

\PY{n}{numeric\PYZus{}dict3} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{popularity}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{duration\PYZus{}ms}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{danceability}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{energy}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loudness}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{speechiness}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{acousticness}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{instrumentalness}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{liveness}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valence}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tempo}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mi}{0}
\PY{p}{\PYZcb{}}

\PY{n}{highest\PYZus{}r2} \PY{o}{=} \PY{l+m+mi}{0}
\PY{n}{highest\PYZus{}r2\PYZus{}genre} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{highest\PYZus{}r2\PYZus{}response} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}

\PY{k}{for} \PY{n}{genre} \PY{o+ow}{in} \PY{n}{grouped\PYZus{}genre\PYZus{}groups}\PY{p}{:}
    \PY{n}{df} \PY{o}{=} \PY{n}{grouped\PYZus{}genre\PYZus{}groups}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{n}{genre}\PY{p}{,} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{n}{numeric\PYZus{}df} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{select\PYZus{}dtypes}\PY{p}{(}\PY{n}{include}\PY{o}{=}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{number}\PY{p}{]}\PY{p}{)}
    \PY{n}{numeric\PYZus{}df} \PY{o}{=} \PY{n}{numeric\PYZus{}df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{o}{\PYZti{}}\PY{n}{numeric\PYZus{}df}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{isin}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mode}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{key}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{time\PYZus{}signature}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{p}{]}

    \PY{n}{linear\PYZus{}reg} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Select features}
    \PY{n}{selector} \PY{o}{=} \PY{n}{SequentialFeatureSelector}\PY{p}{(}
            \PY{n}{linear\PYZus{}reg}\PY{p}{,}
            \PY{n}{n\PYZus{}features\PYZus{}to\PYZus{}select}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
            \PY{n}{direction}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{forward}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
            \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
            \PY{n}{cv} \PY{o}{=} \PY{l+m+mi}{5}
        \PY{p}{)}

    \PY{n}{max\PYZus{}train\PYZus{}r2} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{max\PYZus{}validation\PYZus{}r2} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{max\PYZus{}response} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}

    \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{numeric\PYZus{}df}\PY{o}{.}\PY{n}{columns}\PY{p}{:}
        \PY{n}{random\PYZus{}seed} \PY{o}{=} \PY{l+m+mi}{42}
        \PY{n}{response} \PY{o}{=} \PY{n}{x}
        \PY{c+c1}{\PYZsh{} Splitting the data}
        \PY{c+c1}{\PYZsh{} First split: separate out 20\PYZpc{} for the test set}
        \PY{n}{train\PYZus{}val}\PY{p}{,} \PY{n}{test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{numeric\PYZus{}df}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}seed}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Second split: separate remaining 80\PYZpc{} into 60\PYZpc{} training and 40\PYZpc{} validation}
        \PY{n}{train}\PY{p}{,} \PY{n}{val} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{train\PYZus{}val}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.25}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}seed}\PY{p}{)}  \PY{c+c1}{\PYZsh{} 0.25 * 0.8 = 0.2}

        \PY{c+c1}{\PYZsh{} Reshape the data to fit the model}
        \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{train}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{n}{response}\PY{p}{)}
        \PY{n}{y} \PY{o}{=} \PY{n}{train}\PY{p}{[}\PY{n}{response}\PY{p}{]}


        \PY{n}{selector}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y}\PY{p}{)}
        \PY{n}{selected\PYZus{}features} \PY{o}{=} \PY{n}{selector}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names\PYZus{}out}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Transform data sets}
        \PY{n}{X} \PY{o}{=} \PY{n}{selector}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}

        \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{selector}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{test}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{n}{response}\PY{p}{)}\PY{p}{)}
        \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{test}\PY{p}{[}\PY{n}{response}\PY{p}{]}

        \PY{n}{X\PYZus{}val} \PY{o}{=} \PY{n}{selector}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{val}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{n}{response}\PY{p}{)}\PY{p}{)}
        \PY{n}{y\PYZus{}val} \PY{o}{=} \PY{n}{val}\PY{p}{[}\PY{n}{response}\PY{p}{]}

        \PY{c+c1}{\PYZsh{} Fit the Linear Regression model}
        \PY{n}{linear\PYZus{}reg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}
        \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{linear\PYZus{}reg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Calculating Evaluation Metrics:}
        \PY{n}{y\PYZus{}val\PYZus{}pred} \PY{o}{=} \PY{n}{linear\PYZus{}reg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Calculate metrics for the training set}
        \PY{n}{train\PYZus{}mse} \PY{o}{=} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)} \PY{c+c1}{\PYZsh{} Mean squared error}
        \PY{n}{train\PYZus{}rmse} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{train\PYZus{}mse}\PY{p}{)} \PY{c+c1}{\PYZsh{} Root mean squared error}
        \PY{n}{train\PYZus{}mae} \PY{o}{=} \PY{n}{mean\PYZus{}absolute\PYZus{}error}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)} \PY{c+c1}{\PYZsh{} Mean average error}
        \PY{n}{train\PYZus{}mad} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{y} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Mean absolute deviation}
        \PY{n}{train\PYZus{}r2} \PY{o}{=} \PY{n}{r2\PYZus{}score}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)} \PY{c+c1}{\PYZsh{} R\PYZca{}2 (coefficient of determination)}

        \PY{c+c1}{\PYZsh{} Calculate metrics for the validation set}
        \PY{n}{val\PYZus{}mse} \PY{o}{=} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val\PYZus{}pred}\PY{p}{)}
        \PY{n}{val\PYZus{}rmse} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{val\PYZus{}mse}\PY{p}{)}
        \PY{n}{val\PYZus{}mae} \PY{o}{=} \PY{n}{mean\PYZus{}absolute\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val\PYZus{}pred}\PY{p}{)}
        \PY{n}{val\PYZus{}mad} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{y\PYZus{}val} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}val\PYZus{}pred}\PY{p}{)}\PY{p}{)}
        \PY{n}{val\PYZus{}r2} \PY{o}{=} \PY{n}{r2\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val\PYZus{}pred}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Print Results:}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{genre}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ x }\PY{l+s+si}{\PYZob{}}\PY{n}{response}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ \PYZhy{} Training MSE: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}mse}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, rMSE: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}rmse}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, MAE: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}mae}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, MAD: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}mad}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, R²: }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}r2}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{genre}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ x }\PY{l+s+si}{\PYZob{}}\PY{n}{response}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ \PYZhy{} Validation MSE: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}mse}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, rMSE: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}rmse}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, MAE: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}mae}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, MAD: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}mad}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, R²: }\PY{l+s+si}{\PYZob{}}\PY{n}{val\PYZus{}r2}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

        \PY{k}{if} \PY{n}{val\PYZus{}r2} \PY{o}{\PYZgt{}} \PY{n}{max\PYZus{}validation\PYZus{}r2}\PY{p}{:}
            \PY{n}{max\PYZus{}train\PYZus{}r2} \PY{o}{=} \PY{n}{train\PYZus{}r2}
            \PY{n}{max\PYZus{}validation\PYZus{}r2} \PY{o}{=} \PY{n}{val\PYZus{}r2}
            \PY{n}{max\PYZus{}response} \PY{o}{=} \PY{n}{x}

        \PY{c+c1}{\PYZsh{} bias variance}
        \PY{n}{avg\PYZus{}expected\PYZus{}loss}\PY{p}{,} \PY{n}{avg\PYZus{}bias}\PY{p}{,} \PY{n}{avg\PYZus{}var} \PY{o}{=} \PY{n}{bias\PYZus{}variance\PYZus{}decomp}\PY{p}{(}
            \PY{n}{linear\PYZus{}reg}\PY{p}{,}
            \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{o}{.}\PY{n}{values}\PY{p}{,}
            \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{values}\PY{p}{,}
            \PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mse}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
            \PY{n}{random\PYZus{}seed}\PY{o}{=}\PY{l+m+mi}{123}
        \PY{p}{)}

        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{genre}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ x }\PY{l+s+si}{\PYZob{}}\PY{n}{response}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ \PYZhy{} Loss, Variance, and Bias: }\PY{l+s+si}{\PYZob{}}\PY{n}{avg\PYZus{}expected\PYZus{}loss}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, }\PY{l+s+si}{\PYZob{}}\PY{n}{avg\PYZus{}bias}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, }\PY{l+s+si}{\PYZob{}}\PY{n}{avg\PYZus{}var}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Response Variable with the Highest Validation R\PYZca{}2 for }\PY{l+s+si}{\PYZob{}}\PY{n}{genre}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{: }\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}response}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Train R\PYZca{}2 for }\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}response}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ in }\PY{l+s+si}{\PYZob{}}\PY{n}{genre}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{: }\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}train\PYZus{}r2}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Validation R\PYZca{}2 for }\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}response}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ in }\PY{l+s+si}{\PYZob{}}\PY{n}{genre}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{: }\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}validation\PYZus{}r2}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{numeric\PYZus{}dict3}\PY{p}{[}\PY{n}{max\PYZus{}response}\PY{p}{]}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Storing the variable / genre with the highest R\PYZca{}2 values}
    \PY{k}{if}\PY{p}{(}\PY{n}{max\PYZus{}validation\PYZus{}r2} \PY{o}{\PYZgt{}} \PY{n}{highest\PYZus{}r2}\PY{p}{)}\PY{p}{:}
        \PY{n}{highest\PYZus{}r2} \PY{o}{=} \PY{n}{max\PYZus{}validation\PYZus{}r2}
        \PY{n}{highest\PYZus{}r2\PYZus{}genre} \PY{o}{=} \PY{n}{genre}
        \PY{n}{highest\PYZus{}r2\PYZus{}response} \PY{o}{=} \PY{n}{response}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{numeric\PYZus{}dict}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The highest Validation R\PYZca{}2 was between }\PY{l+s+si}{\PYZob{}}\PY{n}{genre}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ and }\PY{l+s+si}{\PYZob{}}\PY{n}{response}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, with a score of }\PY{l+s+si}{\PYZob{}}\PY{n}{highest\PYZus{}r2}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Output:
\texttt{The\ highest\ Validation\ R\^{}2\ was\ between\ world\ and\ tempo,\ with\ a\ score\ of\ 0.8921830467528595.}

    Compared to the results of our linear regression with the 114 genres,
most genres still seemed like energy as the response variable had the
highest correlation once again. And similar to before, the highest
correlation was within the world genre with tempo as the response
variable, though the validation R\^{}2 dropped in comparison to the
linear regression with 114 genres (from 0.913 to 0.892).

Because we weren't necessarily thrying to predict anything with our
linear and polynomial regression models, and since most of them didn't
seem to be doing so well that it suggested overfitting, we didn't think
we needed to do regularization on our models. We used these models more
for exploration rather than prediction.

However, if interested, we did do lasso and ridge regularization with
our exploration of energy as a response variable, which can be found in
the week 3 check-in.

    \subsection{Logistic Regression}\label{logistic-regression}

For our logistic regression, we wanted to try to see if we could
classify a track's genre based on other variables.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{111}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{label\PYZus{}encoder} \PY{o}{=} \PY{n}{LabelEncoder}\PY{p}{(}\PY{p}{)}
\PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{grouped\PYZus{}spotify}\PY{p}{[}\PY{n}{response\PYZus{}variable}\PY{p}{]}\PY{p}{)}
\PY{n}{Y\PYZus{}train} \PY{o}{=} \PY{n}{grouped\PYZus{}spotify\PYZus{}train}\PY{p}{[}\PY{n}{response\PYZus{}variable}\PY{p}{]}
\PY{n}{Y\PYZus{}train} \PY{o}{=} \PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{Y\PYZus{}train}\PY{p}{)}

\PY{n}{Y\PYZus{}test} \PY{o}{=} \PY{n}{grouped\PYZus{}spotify\PYZus{}test}\PY{p}{[}\PY{n}{response\PYZus{}variable}\PY{p}{]}
\PY{n}{Y\PYZus{}test} \PY{o}{=} \PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{)}

\PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{ohe\PYZus{}grouped\PYZus{}column\PYZus{}transformer\PYZus{}wo\PYZus{}genre}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{grouped\PYZus{}spotify\PYZus{}train}\PY{p}{)}
\PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{ohe\PYZus{}grouped\PYZus{}column\PYZus{}transformer\PYZus{}wo\PYZus{}genre}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{grouped\PYZus{}spotify\PYZus{}test}\PY{p}{)}

\PY{n}{scaler} \PY{o}{=} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}
\PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{scaler}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
\PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{scaler}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{} logistic regression}
\PY{n}{logreg} \PY{o}{=} \PY{n}{LogisticRegression}\PY{p}{(}\PY{n}{solver}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lbfgs}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{)}
\PY{n}{logreg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{111}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
LogisticRegression(max\_iter=1000)
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{110}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Evaluate the model}
\PY{n}{Y\PYZus{}pred} \PY{o}{=} \PY{n}{logreg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\PY{n}{Y\PYZus{}pred\PYZus{}proba} \PY{o}{=} \PY{n}{logreg}\PY{o}{.}\PY{n}{predict\PYZus{}proba}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}

\PY{n}{report} \PY{o}{=} \PY{n}{classification\PYZus{}report}\PY{p}{(}
    \PY{n}{Y\PYZus{}test}\PY{p}{,}
    \PY{n}{Y\PYZus{}pred}\PY{p}{,}
    \PY{n}{target\PYZus{}names}\PY{o}{=}\PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{classes\PYZus{}}\PY{p}{,}
\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{report}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
              precision    recall  f1-score   support

   classical       0.36      0.11      0.16       607
  electronic       0.44      0.68      0.54      4839
        folk       0.31      0.18      0.23      1218
     hip-hop       0.50      0.00      0.00       767
        jazz       0.11      0.00      0.00      1242
       metal       0.52      0.57      0.54      1203
        misc       0.43      0.36      0.39      3504
         pop       0.28      0.08      0.13      2020
        rock       0.30      0.28      0.29      2828
       world       0.34      0.52      0.41      4538

    accuracy                           0.39     22766
   macro avg       0.36      0.28      0.27     22766
weighted avg       0.37      0.39      0.35     22766

    \end{Verbatim}

    Looking at the evaluation metrics for our logistic regression, it seems
like the precision for our predictions is not very high, with the
highest being jazz, at 52\%.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{104}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} ROC and AUC}
\PY{n}{best\PYZus{}thresholds} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{Y\PYZus{}pred\PYZus{}proba}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{genre} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{classes\PYZus{}}\PY{p}{)}\PY{p}{:}
    \PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{,} \PY{n}{thesholds} \PY{o}{=} \PY{n}{roc\PYZus{}curve}\PY{p}{(}\PY{n}{Y\PYZus{}test} \PY{o}{==} \PY{n}{i}\PY{p}{,} \PY{n}{Y\PYZus{}pred\PYZus{}proba}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{p}{)}
    \PY{n}{auc\PYZus{}score} \PY{o}{=} \PY{n}{auc}\PY{p}{(}\PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{)}
    \PY{n}{best\PYZus{}thresholds}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{thesholds}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{tpr} \PY{o}{\PYZhy{}} \PY{n}{fpr}\PY{p}{)}\PY{p}{]}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{genre}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ (AUC = }\PY{l+s+si}{\PYZob{}}\PY{n}{auc\PYZus{}score}\PY{l+s+si}{:}\PY{l+s+s2}{.2f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} print best threshold}
\PY{n}{best\PYZus{}threshold} \PY{o}{=} \PY{n}{best\PYZus{}thresholds}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best Threshold: }\PY{l+s+si}{\PYZob{}}\PY{n}{best\PYZus{}threshold}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{False Positive Rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{True Positive Rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ROC Curve}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Best Threshold: 0.10861536576803565
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{104}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<matplotlib.legend.Legend at 0x7d7c9c9d4710>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_86_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{87}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} permutation importance}
\PY{n}{perm\PYZus{}importance} \PY{o}{=} \PY{n}{permutation\PYZus{}test\PYZus{}score}\PY{p}{(}
    \PY{n}{logreg}\PY{p}{,}
    \PY{n}{X\PYZus{}test}\PY{p}{,}
    \PY{n}{Y\PYZus{}test}\PY{p}{,}
    \PY{n}{scoring}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{accuracy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,}
    \PY{n}{n\PYZus{}permutations}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,}
    \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}
    \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}
\PY{p}{)}
\PY{n}{score}\PY{p}{,} \PY{n}{permutation\PYZus{}scores}\PY{p}{,} \PY{n}{p\PYZus{}value} \PY{o}{=} \PY{n}{perm\PYZus{}importance}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{88}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Plot permutation test scores}
\PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{permutation\PYZus{}scores}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{edgecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.7}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{axvline}\PY{p}{(}\PY{n}{score}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linestyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Original Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Frequency}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Permutation Scores}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_88_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{107}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} feature importances}
\PY{n}{feature\PYZus{}importances} \PY{o}{=} \PY{n}{permutation\PYZus{}importance}\PY{p}{(}
    \PY{n}{logreg}\PY{p}{,}
    \PY{n}{X\PYZus{}test}\PY{p}{,}
    \PY{n}{Y\PYZus{}test}\PY{p}{,}
    \PY{n}{n\PYZus{}repeats}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,}
    \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}
    \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}
\PY{p}{)}
\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}
    \PY{n}{feature\PYZus{}importances}\PY{o}{.}\PY{n}{importances\PYZus{}mean}\PY{p}{,}
    \PY{n}{index}\PY{o}{=}\PY{n}{ohe\PYZus{}grouped\PYZus{}column\PYZus{}transformer\PYZus{}wo\PYZus{}genre}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names\PYZus{}out}\PY{p}{(}\PY{p}{)}\PY{p}{,}
    \PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Importance}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{107}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                  Importance
key\_0              -0.000539
key\_1               0.001236
key\_2               0.000697
key\_3               0.000407
key\_4               0.000221
key\_5               0.000479
key\_6              -0.000257
key\_7               0.001406
key\_8              -0.000363
key\_9               0.000019
key\_10              0.000014
key\_11              0.000649
mode\_0              0.001288
mode\_1              0.001288
time\_signature\_0    0.000000
time\_signature\_1    0.000172
time\_signature\_3   -0.000278
time\_signature\_4   -0.000046
time\_signature\_5    0.001314
popularity          0.003693
duration\_ms         0.010849
explicit            0.002161
danceability        0.058037
energy              0.032968
key                -0.000184
loudness            0.005119
mode                0.001288
speechiness         0.013135
acousticness        0.066806
instrumentalness    0.020670
liveness            0.005648
valence             0.034678
tempo               0.003171
time\_signature     -0.000537
\end{Verbatim}
\end{tcolorbox}
        
    \subsection{KNN/Decision Trees/Random
Forest}\label{knndecision-treesrandom-forest}

Since one of our main goals with the project was to see if we can
classify or predict genres, KNN, decision trees, and random forests were
good models for exploring this question. We decided to start by building
a simple decision tree, using a grid search to tune the hyperparameters.

Decision trees are a good model for the data, since it is likely that a
classification is not a simple function in any of the features, and
cases overlap heavily.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{35}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{response\PYZus{}variable} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{track\PYZus{}genre}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{Y\PYZus{}train} \PY{o}{=} \PY{n}{grouped\PYZus{}spotify\PYZus{}train}\PY{p}{[}\PY{n}{response\PYZus{}variable}\PY{p}{]}
\PY{n}{Y\PYZus{}test} \PY{o}{=} \PY{n}{grouped\PYZus{}spotify\PYZus{}test}\PY{p}{[}\PY{n}{response\PYZus{}variable}\PY{p}{]}

\PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{grouped\PYZus{}spotify\PYZus{}train}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{n}{response\PYZus{}variable}\PY{p}{,} \PY{o}{*}\PY{n}{string\PYZus{}columns}\PY{p}{]}\PY{p}{)}
\PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{grouped\PYZus{}spotify\PYZus{}test}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{n}{response\PYZus{}variable}\PY{p}{,} \PY{o}{*}\PY{n}{string\PYZus{}columns}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} tuning decision tree hyperparameters}
\PY{n}{dt\PYZus{}parameters} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{criterion}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gini}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{entropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{k+kc}{None}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{30}\PY{p}{,} \PY{l+m+mi}{40}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}split}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{k+kc}{None}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sqrt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{p}{\PYZcb{}}

\PY{n}{dt\PYZus{}classifier} \PY{o}{=} \PY{n}{DecisionTreeClassifier}\PY{p}{(}
    \PY{n}{max\PYZus{}depth}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,}
    \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}
\PY{p}{)}

\PY{n}{grid\PYZus{}search} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}
    \PY{n}{dt\PYZus{}classifier}\PY{p}{,}
    \PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{n}{dt\PYZus{}parameters}\PY{p}{,}
    \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,}
    \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,}
    \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{2}
\PY{p}{)}

\PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
GridSearchCV(cv=5,
             estimator=DecisionTreeClassifier(max\_depth=50, random\_state=42),
             n\_jobs=3,
             param\_grid=\{'criterion': ['gini', 'entropy'],
                         'max\_depth': [None, 10, 20, 30, 40, 50],
                         'max\_features': [None, 'sqrt', 'log2'],
                         'min\_samples\_leaf': [1, 2, 4],
                         'min\_samples\_split': [2, 5, 10]\})
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{64}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{dt\PYZus{}classifier} \PY{o}{=} \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}

\PY{c+c1}{\PYZsh{} predictions for testing}
\PY{n}{Y\PYZus{}pred} \PY{o}{=} \PY{n}{dt\PYZus{}classifier}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Evaluation metrics}
\PY{n}{dt\PYZus{}class\PYZus{}report} \PY{o}{=} \PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{Y\PYZus{}pred}\PY{p}{,} \PY{n}{zero\PYZus{}division}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{dt\PYZus{}conf\PYZus{}matrix} \PY{o}{=} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{Y\PYZus{}pred}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Classification Report: }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{ }\PY{l+s+si}{\PYZob{}}\PY{n}{dt\PYZus{}class\PYZus{}report}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Confusion Matrix: }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{ }\PY{l+s+si}{\PYZob{}}\PY{n}{dt\PYZus{}conf\PYZus{}matrix}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Classification Report:
               precision    recall  f1-score   support

   classical       0.58      0.34      0.43       607
  electronic       0.49      0.62      0.55      4026
        folk       0.37      0.27      0.31      1218
     hip-hop       0.35      0.09      0.14       578
        jazz       0.38      0.27      0.32       830
       metal       0.63      0.54      0.58      1203
        misc       0.48      0.53      0.51      5931
         pop       0.38      0.25      0.30      1825
        rock       0.38      0.25      0.30      2227
       world       0.44      0.56      0.49      4321

    accuracy                           0.46     22766
   macro avg       0.45      0.37      0.39     22766
weighted avg       0.46      0.46      0.45     22766

Confusion Matrix:
 [[ 206   26   13    0   18    0  208   29   24   83]
 [   2 2506   71   46   32   69  655  114  109  422]
 [   2  114  327   10   71    0  267   71   76  280]
 [   0  156    8   52   10   11  105   54   31  151]
 [   2   87   86    2  227    1  101   38   76  210]
 [   0   93    6    5    0  651  283    9  133   23]
 [  89  803  116   17   88  171 3172  170  263 1042]
 [   7  337   74    4   51   37  400  458   79  378]
 [   6  355  113    6   39   78  582   70  554  424]
 [  39  595   80    5   61   22  821  198  101 2399]]
    \end{Verbatim}

    Our decision tree seems to have about 46\% accuracy, meaning it
classified about 46\% of the data correctly. This is better than chance,
but could still be improved. In particular, it seems our decision tree
was best at calculating electronic tracks, with a precision of 62\%,
while it was very bad at classifying hip-hop tracks, with a precision
score of only 9\%.

Based on the confusion matrix, it seems like electronic tracks, when
misclassified, tended to be mislabeled as pop (276 instances) or rock
(265 instances), which may be some indicator towards the similarities or
overlaps with these genres. We also noticed that hip-hop tends to be
mislabelled as electronic, as with hip-hop tracks only 160 were
correctly labelled while 283 were labelled as electronic.

However, it is important to note that when we combined the genres into
10 more generalized genres, we caused an imbalance in samples. For
example, electronic tracks have several more samples than hip-hop ones
(20000 vs 3000) which may influence our decision tree's balance.

To learn more about which features in particular were contributing to
our classification, we looked at the feature importances of our decision
tree.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{66}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Get feature importances}
\PY{n}{feature\PYZus{}importances} \PY{o}{=} \PY{n}{dt\PYZus{}classifier}\PY{o}{.}\PY{n}{feature\PYZus{}importances\PYZus{}}
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{importance} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{feature\PYZus{}importances}\PY{p}{)}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{: }\PY{l+s+si}{\PYZob{}}\PY{n}{importance}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
popularity: 0.2200472160153845
duration\_ms: 0.0852801213153109
explicit: 0.006176993344808496
danceability: 0.11891001330395011
energy: 0.04639268264190049
key: 0.0010495388111708972
loudness: 0.04128858458997459
mode: 0.006500681736710005
speechiness: 0.06524558872054981
acousticness: 0.18262725408120783
instrumentalness: 0.09890842233853107
liveness: 0.014775753707866195
valence: 0.05226490886822944
tempo: 0.05978402867389046
time\_signature: 0.000748211850515286
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{68}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} permutation importance}
\PY{n}{perm\PYZus{}importance} \PY{o}{=} \PY{n}{permutation\PYZus{}importance}\PY{p}{(}
    \PY{n}{dt\PYZus{}classifier}\PY{p}{,}
    \PY{n}{X\PYZus{}test}\PY{p}{,}
    \PY{n}{Y\PYZus{}test}\PY{p}{,}
    \PY{n}{n\PYZus{}repeats}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,}
    \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}
\PY{p}{)}

\PY{n}{perm\PYZus{}importance\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{\PYZob{}}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Feature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Importance Mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{perm\PYZus{}importance}\PY{o}{.}\PY{n}{importances\PYZus{}mean}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Importance Std}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{perm\PYZus{}importance}\PY{o}{.}\PY{n}{importances\PYZus{}std}
\PY{p}{\PYZcb{}}\PY{p}{)}

\PY{n}{perm\PYZus{}importance\PYZus{}df}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{68}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
             Feature  Importance Mean  Importance Std
0         popularity         0.153321        0.002175
1        duration\_ms         0.038263        0.001421
2           explicit         0.002715        0.000277
3       danceability         0.070403        0.001225
4             energy         0.026368        0.001298
5                key         0.000020        0.000083
6           loudness         0.021769        0.001009
7               mode         0.002462        0.000285
8        speechiness         0.024363        0.000918
9       acousticness         0.119643        0.002600
10  instrumentalness         0.058062        0.001181
11          liveness         0.004871        0.000420
12           valence         0.028775        0.001081
13             tempo         0.024633        0.000823
14    time\_signature         0.000198        0.000110
\end{Verbatim}
\end{tcolorbox}
        
    Out of this, it seems like acousticness, popularity, and danceability
have the highest importance when it comes to our decision tree. On the
other hand, some of the time signatures, keys, and modes didn't really
seem to be as key in feature importance.

To add more complexity to our model in hopes of improving its accuracy,
we decided to try to train a random forest model on our data as well. We
used a grid search again, but this time, we reduced the number of
features in the search due to the higher complexity of the model
requiring much longer time to do tuning.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{rf\PYZus{}parameters} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{30}\PY{p}{]}\PY{p}{,}
\PY{p}{\PYZcb{}}

\PY{c+c1}{\PYZsh{} Random forest}
\PY{n}{rf\PYZus{}classifier} \PY{o}{=} \PY{n}{RandomForestClassifier}\PY{p}{(}
    \PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,}
    \PY{n}{max\PYZus{}depth}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,}
    \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{,}
\PY{p}{)}

\PY{n}{grid\PYZus{}search} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}
    \PY{n}{estimator}\PY{o}{=}\PY{n}{rf\PYZus{}classifier}\PY{p}{,}
    \PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{n}{rf\PYZus{}parameters}\PY{p}{,}
    \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,}
    \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}
    \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{2}
\PY{p}{)}
\PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Fitting 5 folds for each of 6 candidates, totalling 30 fits
[CV] END {\ldots}max\_depth=10, n\_estimators=100; total time=   6.3s
[CV] END {\ldots}max\_depth=10, n\_estimators=100; total time=   6.3s
[CV] END {\ldots}max\_depth=10, n\_estimators=100; total time=   6.3s
[CV] END {\ldots}max\_depth=10, n\_estimators=100; total time=   6.3s
[CV] END {\ldots}max\_depth=10, n\_estimators=100; total time=   6.2s
[CV] END {\ldots}max\_depth=10, n\_estimators=200; total time=  12.4s
[CV] END {\ldots}max\_depth=10, n\_estimators=200; total time=  12.6s
[CV] END {\ldots}max\_depth=10, n\_estimators=200; total time=  12.5s
[CV] END {\ldots}max\_depth=10, n\_estimators=200; total time=  12.5s
[CV] END {\ldots}max\_depth=10, n\_estimators=200; total time=  12.4s
[CV] END {\ldots}max\_depth=20, n\_estimators=100; total time=  10.8s
[CV] END {\ldots}max\_depth=20, n\_estimators=100; total time=  10.8s
[CV] END {\ldots}max\_depth=20, n\_estimators=100; total time=  10.9s
[CV] END {\ldots}max\_depth=20, n\_estimators=100; total time=  10.8s
[CV] END {\ldots}max\_depth=20, n\_estimators=100; total time=  10.9s
[CV] END {\ldots}max\_depth=20, n\_estimators=200; total time=  21.8s
[CV] END {\ldots}max\_depth=20, n\_estimators=200; total time=  21.1s
[CV] END {\ldots}max\_depth=20, n\_estimators=200; total time=  21.0s
[CV] END {\ldots}max\_depth=20, n\_estimators=200; total time=  20.9s
[CV] END {\ldots}max\_depth=20, n\_estimators=200; total time=  20.7s
[CV] END {\ldots}max\_depth=30, n\_estimators=100; total time=  11.0s
[CV] END {\ldots}max\_depth=30, n\_estimators=100; total time=  11.1s
[CV] END {\ldots}max\_depth=30, n\_estimators=100; total time=  11.4s
[CV] END {\ldots}max\_depth=30, n\_estimators=100; total time=  11.5s
[CV] END {\ldots}max\_depth=30, n\_estimators=100; total time=  11.2s
[CV] END {\ldots}max\_depth=30, n\_estimators=200; total time=  22.3s
[CV] END {\ldots}max\_depth=30, n\_estimators=200; total time=  21.9s
[CV] END {\ldots}max\_depth=30, n\_estimators=200; total time=  22.0s
[CV] END {\ldots}max\_depth=30, n\_estimators=200; total time=  22.3s
[CV] END {\ldots}max\_depth=30, n\_estimators=200; total time=  22.5s
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
GridSearchCV(cv=5,
             estimator=RandomForestClassifier(max\_depth=20, random\_state=42),
             n\_jobs=1,
             param\_grid=\{'max\_depth': [10, 20, 30], 'n\_estimators': [100, 200]\},
             verbose=2)
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{41}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}\PY{p}{)}
\PY{n}{rf\PYZus{}classifier} \PY{o}{=} \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}
\PY{n}{Y\PYZus{}pred\PYZus{}rf} \PY{o}{=} \PY{n}{rf\PYZus{}classifier}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\{'max\_depth': 20, 'n\_estimators': 200\}
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{46}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Evaluation metrics}
\PY{n}{rf\PYZus{}train\PYZus{}accuracy} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}train}\PY{p}{,} \PY{n}{rf\PYZus{}classifier}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{)}
\PY{n}{rf\PYZus{}accuracy} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{Y\PYZus{}pred\PYZus{}rf}\PY{p}{)}
\PY{n}{rf\PYZus{}class\PYZus{}report} \PY{o}{=} \PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{Y\PYZus{}pred\PYZus{}rf}\PY{p}{,} \PY{n}{zero\PYZus{}division}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{rf\PYZus{}conf\PYZus{}matrix} \PY{o}{=} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{Y\PYZus{}pred\PYZus{}rf}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train accuracy: }\PY{l+s+si}{\PYZob{}}\PY{n}{rf\PYZus{}train\PYZus{}accuracy}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy: }\PY{l+s+si}{\PYZob{}}\PY{n}{rf\PYZus{}accuracy}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Classification Report: }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{ }\PY{l+s+si}{\PYZob{}}\PY{n}{rf\PYZus{}class\PYZus{}report}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Confusion Matrix:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{rf\PYZus{}conf\PYZus{}matrix}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Train accuracy: 0.9188239428370621
Accuracy: 0.5578054994289731
Classification Report:
               precision    recall  f1-score   support

   classical       0.60      0.36      0.45       607
  electronic       0.66      0.70      0.68      4026
        folk       0.68      0.55      0.61      1218
     hip-hop       0.30      0.08      0.12       578
        jazz       0.60      0.35      0.45       830
       metal       0.65      0.62      0.63      1203
        misc       0.50      0.65      0.57      5931
         pop       0.48      0.34      0.40      1825
        rock       0.48      0.33      0.39      2227
       world       0.55      0.62      0.59      4321

    accuracy                           0.56     22766
   macro avg       0.55      0.46      0.49     22766
weighted avg       0.55      0.56      0.55     22766

Confusion Matrix:
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{46}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
array([[ 219,   11,   10,    0,    5,    4,  249,   18,   14,   77],
       [   6, 2802,   12,   13,   27,   43,  677,   83,   63,  300],
       [   1,   34,  672,    6,   10,    1,  224,   55,   48,  167],
       [   0,  158,    7,   45,    9,    5,   96,   51,   39,  168],
       [   2,   66,   21,    6,  294,    2,  147,   48,   73,  171],
       [   1,   46,    3,    0,    3,  746,  267,    8,  109,   20],
       [  84,  522,   74,    1,   40,  178, 3884,  195,  282,  671],
       [   1,  200,   49,   31,   26,   27,  515,  613,   78,  285],
       [  16,  119,   68,   16,   36,  119,  751,   59,  746,  297],
       [  32,  286,   79,   30,   36,   28,  901,  136,  115, 2678]])
\end{Verbatim}
\end{tcolorbox}
        
    Our random forest model does seem to do much better than just one
decision tree, with an increase of accuracy from \textasciitilde45\% to
\textasciitilde56\%. Additionally, the individual genres' precisions for
the most part seemed to get better. This time, classical and metal
genres had the highest precision, which, when thinking back to our EDA
where the two variables seemed to stand out, makes sense. Similar with
decision trees, the model was not as good at classifying hip-hop tracks.

Looking again at feature importance for our random forest:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} permutation importance}
\PY{n}{perm\PYZus{}importance\PYZus{}rf} \PY{o}{=} \PY{n}{permutation\PYZus{}importance}\PY{p}{(}
    \PY{n}{rf\PYZus{}classifier}\PY{p}{,}
    \PY{n}{X\PYZus{}test}\PY{p}{,}
    \PY{n}{Y\PYZus{}test}\PY{p}{,}
    \PY{n}{n\PYZus{}repeats}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,}
    \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{,}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{44}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{perm\PYZus{}importance\PYZus{}rf\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{\PYZob{}}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Feature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Importance Mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{perm\PYZus{}importance\PYZus{}rf}\PY{o}{.}\PY{n}{importances\PYZus{}mean}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Importance Std}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{perm\PYZus{}importance\PYZus{}rf}\PY{o}{.}\PY{n}{importances\PYZus{}std}
\PY{p}{\PYZcb{}}\PY{p}{)}

\PY{n}{perm\PYZus{}importance\PYZus{}rf\PYZus{}df}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{44}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
             Feature  Importance Mean  Importance Std
0         popularity         0.119450        0.002112
1        duration\_ms         0.034292        0.001582
2           explicit         0.002846        0.000426
3       danceability         0.053180        0.001849
4             energy         0.030636        0.001262
5                key        -0.000356        0.000504
6           loudness         0.016490        0.001054
7               mode         0.004507        0.000626
8        speechiness         0.027034        0.001284
9       acousticness         0.079966        0.001813
10  instrumentalness         0.055049        0.001748
11          liveness         0.004076        0.000793
12           valence         0.031598        0.001160
13             tempo         0.014998        0.001157
14    time\_signature         0.000283        0.000386
\end{Verbatim}
\end{tcolorbox}
        
    With our random forest, the importance of specific variables changed
slightly from our decision tree, but the most important features,
popularity, danceability, and acousticness, stayed about the same.

We also looked into training a KNN model for classification as well, to
see how it may do against our decision tree and random forest.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{132}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} KNN}
\PY{n}{knn\PYZus{}classifier} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
\PY{n}{knn\PYZus{}classifier}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Make predictions}
\PY{n}{Y\PYZus{}pred\PYZus{}knn} \PY{o}{=} \PY{n}{knn\PYZus{}classifier}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Evaluate}
\PY{n}{knn\PYZus{}accuracy} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{Y\PYZus{}pred\PYZus{}knn}\PY{p}{)}
\PY{n}{knn\PYZus{}class\PYZus{}report} \PY{o}{=} \PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{Y\PYZus{}pred\PYZus{}knn}\PY{p}{,} \PY{n}{zero\PYZus{}division}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{knn\PYZus{}conf\PYZus{}matrix} \PY{o}{=} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{Y\PYZus{}pred\PYZus{}knn}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy: }\PY{l+s+si}{\PYZob{}}\PY{n}{knn\PYZus{}accuracy}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Classification Report: }\PY{l+s+si}{\PYZob{}}\PY{n}{knn\PYZus{}class\PYZus{}report}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Confusion Matrix:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{knn\PYZus{}conf\PYZus{}matrix}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy: 0.29438636563296144
Classification Report:               precision    recall  f1-score   support

   classical       0.09      0.10      0.10       607
  electronic       0.31      0.47      0.37      4026
        folk       0.25      0.24      0.25      1218
     hip-hop       0.11      0.06      0.08       578
        jazz       0.41      0.29      0.34       830
       metal       0.13      0.08      0.10      1203
        misc       0.32      0.39      0.35      5931
         pop       0.19      0.11      0.14      1825
        rock       0.27      0.17      0.21      2227
       world       0.33      0.28      0.31      4321

    accuracy                           0.29     22766
   macro avg       0.24      0.22      0.22     22766
weighted avg       0.28      0.29      0.28     22766

Confusion Matrix:
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{132}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
array([[  59,  143,   29,    9,    6,   26,  221,   14,   24,   76],
       [  79, 1875,  119,   47,   48,   84,  973,  160,  150,  491],
       [  33,  252,  290,   21,   26,   32,  338,   33,   45,  148],
       [  11,  150,   24,   35,   17,   21,  162,   34,   31,   93],
       [  13,  159,   26,   14,  242,   18,  164,   44,   52,   98],
       [  41,  268,   44,   22,   16,   95,  416,   46,   87,  168],
       [ 178, 1431,  238,   62,   95,  213, 2309,  248,  346,  811],
       [  50,  421,   96,   28,   35,   59,  578,  193,   98,  267],
       [  56,  440,  107,   23,   59,   84,  715,   87,  386,  270],
       [ 104, 1004,  173,   53,   43,  115, 1274,  139,  198, 1218]])
\end{Verbatim}
\end{tcolorbox}
        
    Unfortunately, our KNN model seems to be a lot worse than both our
decision tree and random forest, with an accuracy of only
\textasciitilde29\%. Classical, hip-hop, and metal do the worst on this,
likely because of the class (genre) imbalance we have from our
groupings. KNN doesn't seem to be a very good model for our data.

    \subsection{PCA/Clustering}\label{pcaclustering}

We tried to conduct PCA and clustering on our data to see if
dimensionality reduction or investigation into underlying structures
like clusters may relate or help us investigate genre. We tried
dimensionality reduction with PCA first.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{155}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} data pre\PYZhy{}processing}
\PY{n}{response\PYZus{}variable} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{track\PYZus{}genre}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{categorical\PYZus{}columns} \PY{o}{=} \PY{p}{[}\PY{n}{x} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{categorical\PYZus{}columns} \PY{k}{if} \PY{n}{x} \PY{o}{!=} \PY{n}{response\PYZus{}variable}\PY{p}{]}

\PY{c+c1}{\PYZsh{} drop string columns and response variable}
\PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{ohe\PYZus{}grouped\PYZus{}column\PYZus{}transformer\PYZus{}wo\PYZus{}genre}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{grouped\PYZus{}spotify\PYZus{}train}\PY{p}{)}
\PY{n}{scaler} \PY{o}{=} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}
\PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{scaler}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
\PY{n}{Y\PYZus{}train} \PY{o}{=} \PY{n}{grouped\PYZus{}spotify\PYZus{}train}\PY{p}{[}\PY{n}{response\PYZus{}variable}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{156}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} do PCA on X}
\PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{10}
\PY{n}{pca} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{)}
\PY{n}{X\PYZus{}pca} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
\PY{n}{X\PYZus{}pca\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X\PYZus{}pca}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC}\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{p}{)}\PY{p}{]}\PY{p}{)}

\PY{n}{explained\PYZus{}variance} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{explained\PYZus{}variance\PYZus{}ratio\PYZus{}}

\PY{c+c1}{\PYZsh{} Check explained variance ratio for each component}
\PY{n}{explained\PYZus{}variance} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{explained\PYZus{}variance\PYZus{}ratio\PYZus{}}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Explained variance ratio for each component: }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZob{}}\PY{n}{explained\PYZus{}variance}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cumulative explained variance: }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZob{}}\PY{n}{np}\PY{o}{.}\PY{n}{cumsum}\PY{p}{(}\PY{n}{explained\PYZus{}variance}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{explained\PYZus{}variance}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{explained\PYZus{}variance}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linestyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Scree Plot}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Principal Component}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Explained Variance Ratio}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{explained\PYZus{}variance}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Explained variance ratio for each component:
 [0.10836837 0.09131586 0.06138773 0.05869306 0.04588949 0.0434164
 0.0361926  0.03418232 0.03292623 0.03291811]

Cumulative explained variance:
 [0.10836837 0.19968423 0.26107196 0.31976503 0.36565451 0.40907091
 0.44526351 0.47944583 0.51237206 0.54529017]
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_109_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Based on the Scree Plot, it seems like taking the first 3 or 4 principal
components would be best for still covering a good amount of variance
(\textasciitilde0.18 cumulative) and also keeping it simple for easier
visualizations. We plotted the first two PCs and colored them by genre
to see if there were any interesting relationships.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{157}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{label\PYZus{}encoder} \PY{o}{=} \PY{n}{LabelEncoder}\PY{p}{(}\PY{p}{)}
\PY{n}{Y\PYZus{}encoded} \PY{o}{=} \PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{Y\PYZus{}train}\PY{p}{)}

\PY{c+c1}{\PYZsh{} plot the first two principal components}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{)}\PY{p}{)}
\PY{n}{scatter} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}
    \PY{n}{X\PYZus{}pca\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{n}{X\PYZus{}pca\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{n}{c}\PY{o}{=}\PY{n}{Y\PYZus{}encoded}\PY{p}{,}
    \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{viridis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}
\PY{p}{)}
\PY{c+c1}{\PYZsh{} label by Y\PYZus{}encoded}
\PY{n}{cbar} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{colorbar}\PY{p}{(}\PY{n}{scatter}\PY{p}{)}
\PY{n}{cbar}\PY{o}{.}\PY{n}{set\PYZus{}label}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Genre}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{cbar}\PY{o}{.}\PY{n}{set\PYZus{}ticks}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{classes\PYZus{}}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n}{cbar}\PY{o}{.}\PY{n}{set\PYZus{}ticklabels}\PY{p}{(}\PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{classes\PYZus{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PCA of Spotify Data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_111_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The points do seem like they're getting clustered and colored in some
way on the PCA graph which is interesting, but it also looks like there
are a lot of overlapping dots, so we may be losing some important
dimensions.

We decided to do a regression on the PCA transformed data to see if the
PCA improved the fit at all.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{159}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} do a regression using PCA transformed data}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k+kn}{import} \PY{n}{LogisticRegression}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{accuracy\PYZus{}score}

\PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{ohe\PYZus{}grouped\PYZus{}column\PYZus{}transformer\PYZus{}wo\PYZus{}genre}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{grouped\PYZus{}spotify\PYZus{}test}\PY{p}{)}
\PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{scaler}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC}\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\PY{n}{Y\PYZus{}test} \PY{o}{=} \PY{n}{grouped\PYZus{}spotify\PYZus{}test}\PY{p}{[}\PY{n}{response\PYZus{}variable}\PY{p}{]}

\PY{n}{model} \PY{o}{=} \PY{n}{LogisticRegression}\PY{p}{(}\PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{)}
\PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}pca\PYZus{}df}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{)}
\PY{n}{Y\PYZus{}pred} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\PY{n}{Y\PYZus{}pred\PYZus{}encoded} \PY{o}{=} \PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{Y\PYZus{}pred}\PY{p}{)}

\PY{n}{accuracy} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{Y\PYZus{}pred}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy: }\PY{l+s+si}{\PYZob{}}\PY{n}{accuracy}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} plot it}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{)}\PY{p}{)}
\PY{n}{scatter} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}
    \PY{n}{X\PYZus{}test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{n}{X\PYZus{}test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{n}{c}\PY{o}{=}\PY{n}{Y\PYZus{}pred\PYZus{}encoded}\PY{p}{,}
    \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{viridis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}
\PY{p}{)}
\PY{c+c1}{\PYZsh{} label by Y\PYZus{}encoded}
\PY{n}{cbar} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{colorbar}\PY{p}{(}\PY{n}{scatter}\PY{p}{)}
\PY{n}{cbar}\PY{o}{.}\PY{n}{set\PYZus{}label}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Genre}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{cbar}\PY{o}{.}\PY{n}{set\PYZus{}ticks}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{classes\PYZus{}}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n}{cbar}\PY{o}{.}\PY{n}{set\PYZus{}ticklabels}\PY{p}{(}\PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{classes\PYZus{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PCA of Spotify Data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy: 0.3139769832205921
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_113_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Though the accuracy isn't very high with our logistic regression fit,
the graph reminded us of what Gaussian Mixture Models tend to look like,
so we attempted applying a GMM to the PCA transformed data as well.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{161}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} gaussian mixture model}
\PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{10}
\PY{n}{gmm} \PY{o}{=} \PY{n}{GaussianMixture}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{)}
\PY{n}{gmm}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}pca\PYZus{}df}\PY{p}{)}
\PY{n}{Y\PYZus{}pred} \PY{o}{=} \PY{n}{gmm}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}pca\PYZus{}df}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{)}\PY{p}{)}
\PY{n}{scatter} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}
    \PY{n}{X\PYZus{}pca\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{n}{X\PYZus{}pca\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{n}{c}\PY{o}{=}\PY{n}{Y\PYZus{}pred}\PY{p}{,}
    \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{viridis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}
\PY{p}{)}
\PY{n}{cbar} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{colorbar}\PY{p}{(}\PY{n}{scatter}\PY{p}{)}
\PY{n}{cbar}\PY{o}{.}\PY{n}{set\PYZus{}label}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Cluster}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{cbar}\PY{o}{.}\PY{n}{set\PYZus{}ticks}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{p}{)}\PY{p}{)}
\PY{n}{cbar}\PY{o}{.}\PY{n}{set\PYZus{}ticklabels}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{p}{)}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Gaussian Mixture Model of Spotify Data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_115_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{162}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} evaluate the model}
\PY{n}{sil\PYZus{}score} \PY{o}{=} \PY{n}{silhouette\PYZus{}score}\PY{p}{(}\PY{n}{X\PYZus{}pca\PYZus{}df}\PY{p}{,} \PY{n}{Y\PYZus{}pred}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{sil\PYZus{}score}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
0.18532988287647995
    \end{Verbatim}

    While the GMM has a pretty nice graph that almost looks right, the
silhouette score is not good at all. We decided to try it again with
just two genres, pop and not pop (other) to see if it might improve the
score.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{163}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Trying the data but with 2 classes: pop and other}
\PY{n}{Y\PYZus{}binary} \PY{o}{=} \PY{n}{Y\PYZus{}train}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pop}\PY{l+s+s1}{\PYZsq{}} \PY{k}{if} \PY{n}{x} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pop}\PY{l+s+s1}{\PYZsq{}} \PY{k}{else} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{misc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} do GMM}
\PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{2}
\PY{n}{gmm} \PY{o}{=} \PY{n}{GaussianMixture}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{)}
\PY{n}{gmm}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}pca\PYZus{}df}\PY{p}{)}
\PY{n}{Y\PYZus{}pred} \PY{o}{=} \PY{n}{gmm}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}pca\PYZus{}df}\PY{p}{)}


\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{)}\PY{p}{)}
\PY{n}{scatter} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}
    \PY{n}{X\PYZus{}pca\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{n}{X\PYZus{}pca\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{n}{c}\PY{o}{=}\PY{n}{Y\PYZus{}pred}\PY{p}{,}
    \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{viridis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}
\PY{p}{)}
\PY{n}{cbar} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{colorbar}\PY{p}{(}\PY{n}{scatter}\PY{p}{)}
\PY{n}{cbar}\PY{o}{.}\PY{n}{set\PYZus{}label}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Cluster}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{cbar}\PY{o}{.}\PY{n}{set\PYZus{}ticks}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{p}{)}\PY{p}{)}
\PY{n}{cbar}\PY{o}{.}\PY{n}{set\PYZus{}ticklabels}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{p}{)}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PC1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Gaussian Mixture Model of Spotify Data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_118_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{164}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Evaluate}
\PY{n}{Y\PYZus{}binary\PYZus{}encoded} \PY{o}{=} \PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{Y\PYZus{}binary}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{sil\PYZus{}score} \PY{o}{=} \PY{n}{silhouette\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}binary\PYZus{}encoded}\PY{p}{,} \PY{n}{Y\PYZus{}pred}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{sil\PYZus{}score}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
-0.030537843090037743
    \end{Verbatim}

    Unfortunately, our silhouette score is still less than zero, indicating
that our clustering with the GMM is not very good. We decided to try
other methods of clustering, K-means and agglomerative, as well to see
if they would be better.

    Note that when working with the original data that has 114 genres, it's
very evenly distributed (1000 samples per genre). This may imply that
the sampling was not random, and was stratified via the genre
subpopulations. This is likely not proportional to the actual song
population, which may lead to bias since underrepresented genres are now
equally represented with overrepresented genres. Since we have no way to
recover original proportions without using external data, we will simply
have to be weary of the results.

    Our goal with clustering is to be able to see if it might be able to
help us in classifying or predicting the genre of the song. We first
start to see if there is any obvious clustering using both agglomerative
(hierarchial) and k-means clustering.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{175}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{ohe\PYZus{}grouped\PYZus{}column\PYZus{}transformer\PYZus{}wo\PYZus{}genre}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{grouped\PYZus{}spotify\PYZus{}train}\PY{p}{)}
\PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{ohe\PYZus{}grouped\PYZus{}column\PYZus{}transformer\PYZus{}wo\PYZus{}genre}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{grouped\PYZus{}spotify\PYZus{}test}\PY{p}{)}

\PY{n}{scaler} \PY{o}{=} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}
\PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}
    \PY{n}{scaler}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{,}
    \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{index}\PY{p}{,}
    \PY{n}{columns}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{columns}
\PY{p}{)}
\PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}
    \PY{n}{scaler}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{,}
    \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{index}\PY{p}{,}
    \PY{n}{columns}\PY{o}{=}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{columns}
\PY{p}{)}

\PY{n}{Y\PYZus{}train} \PY{o}{=} \PY{n}{grouped\PYZus{}spotify\PYZus{}train}\PY{p}{[}\PY{n}{response\PYZus{}variable}\PY{p}{]}
\PY{n}{Y\PYZus{}test} \PY{o}{=} \PY{n}{grouped\PYZus{}spotify\PYZus{}test}\PY{p}{[}\PY{n}{response\PYZus{}variable}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{172}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} sample the training set to speed up hierarchical clustering}
\PY{n}{sample\PYZus{}size} \PY{o}{=} \PY{l+m+mf}{0.3}
\PY{n}{hierarchical\PYZus{}sample\PYZus{}split} \PY{o}{=} \PY{n}{StratifiedShuffleSplit}\PY{p}{(}
    \PY{n}{n\PYZus{}splits}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}
    \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{sample\PYZus{}size}\PY{p}{,}
    \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}
\PY{p}{)}
\PY{n}{gmm\PYZus{}sample\PYZus{}size} \PY{o}{=} \PY{l+m+mf}{0.8}

\PY{k}{for} \PY{n}{train\PYZus{}index}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n}{hierarchical\PYZus{}sample\PYZus{}split}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{)}\PY{p}{:}
    \PY{n}{X\PYZus{}sample} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{train\PYZus{}index}\PY{p}{]}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
    \PY{n}{Y\PYZus{}sample} \PY{o}{=} \PY{n}{Y\PYZus{}train}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{train\PYZus{}index}\PY{p}{]}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{173}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{n\PYZus{}clusters} \PY{o}{=} \PY{l+m+mi}{10}

\PY{n}{kmeans} \PY{o}{=} \PY{n}{KMeans}\PY{p}{(}
    \PY{n}{n\PYZus{}clusters}\PY{o}{=}\PY{n}{n\PYZus{}clusters}\PY{p}{,}
    \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}
\PY{p}{)}
\PY{n}{hierarchical} \PY{o}{=} \PY{n}{AgglomerativeClustering}\PY{p}{(}
    \PY{n}{n\PYZus{}clusters}\PY{o}{=}\PY{n}{n\PYZus{}clusters}\PY{p}{,}
    \PY{n}{metric}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{euclidean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{n}{linkage}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ward}\PY{l+s+s1}{\PYZsq{}}
\PY{p}{)}
\PY{n}{gmm} \PY{o}{=} \PY{n}{GaussianMixture}\PY{p}{(}
    \PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}clusters}\PY{p}{,}
    \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}
\PY{p}{)}

\PY{n}{hierarchical}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}sample}\PY{p}{)}
\PY{n}{kmeans}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}sample}\PY{p}{)}
\PY{n}{gmm}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{173}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
GaussianMixture(n\_components=10, random\_state=0)
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{176}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}sample}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{hcluster}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{hierarchical}\PY{o}{.}\PY{n}{labels\PYZus{}}
\PY{n}{X\PYZus{}sample}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{kcluster}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{kmeans}\PY{o}{.}\PY{n}{labels\PYZus{}}
\PY{n}{Y\PYZus{}pred\PYZus{}gmm} \PY{o}{=} \PY{n}{gmm}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{} encode Y\PYZus{}test\PYZus{}gmm labels using LabelEncoder}
\PY{n}{label\PYZus{}encoder} \PY{o}{=} \PY{n}{LabelEncoder}\PY{p}{(}\PY{p}{)}
\PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Y\PYZus{}train}\PY{p}{)}
\PY{n}{Y\PYZus{}train\PYZus{}encoded} \PY{o}{=} \PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{Y\PYZus{}train}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Analyze}
\PY{n}{hclust\PYZus{}kclust\PYZus{}ari} \PY{o}{=} \PY{n}{adjusted\PYZus{}rand\PYZus{}score}\PY{p}{(}\PY{n}{X\PYZus{}sample}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{hcluster}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}sample}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{kcluster}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}

\PY{n}{hclust\PYZus{}y\PYZus{}ari} \PY{o}{=} \PY{n}{adjusted\PYZus{}rand\PYZus{}score}\PY{p}{(}\PY{n}{X\PYZus{}sample}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{hcluster}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{Y\PYZus{}sample}\PY{p}{)}
\PY{n}{kclust\PYZus{}y\PYZus{}ari} \PY{o}{=} \PY{n}{adjusted\PYZus{}rand\PYZus{}score}\PY{p}{(}\PY{n}{X\PYZus{}sample}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{kcluster}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{Y\PYZus{}sample}\PY{p}{)}
\PY{n}{kclust\PYZus{}y\PYZus{}ari} \PY{o}{=} \PY{n}{adjusted\PYZus{}rand\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{p}{,} \PY{n}{Y\PYZus{}pred\PYZus{}gmm}\PY{p}{)}

\PY{n}{hclust\PYZus{}silhouette} \PY{o}{=} \PY{n}{silhouette\PYZus{}score}\PY{p}{(}\PY{n}{X\PYZus{}sample}\PY{p}{,} \PY{n}{X\PYZus{}sample}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{hcluster}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{kclust\PYZus{}silhouette} \PY{o}{=} \PY{n}{silhouette\PYZus{}score}\PY{p}{(}\PY{n}{X\PYZus{}sample}\PY{p}{,} \PY{n}{X\PYZus{}sample}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{kcluster}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{gmm\PYZus{}silhouette} \PY{o}{=} \PY{n}{silhouette\PYZus{}score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{Y\PYZus{}pred\PYZus{}gmm}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}\PYZsq{}\PYZsq{}}
\PY{l+s+s1}{Hierarchical clustering vs KMeans clustering ARI: }\PY{l+s+si}{\PYZob{}}\PY{n}{hclust\PYZus{}kclust\PYZus{}ari}\PY{l+s+si}{\PYZcb{}}

\PY{l+s+s1}{Hierarchical clustering vs true labels ARI: }\PY{l+s+si}{\PYZob{}}\PY{n}{hclust\PYZus{}y\PYZus{}ari}\PY{l+s+si}{\PYZcb{}}
\PY{l+s+s1}{KMeans clustering vs true labels ARI: }\PY{l+s+si}{\PYZob{}}\PY{n}{kclust\PYZus{}y\PYZus{}ari}\PY{l+s+si}{\PYZcb{}}
\PY{l+s+s1}{GMM clustering vs true labels ARI: }\PY{l+s+si}{\PYZob{}}\PY{n}{kclust\PYZus{}y\PYZus{}ari}\PY{l+s+si}{\PYZcb{}}

\PY{l+s+s1}{Hierarchical clustering silhouette score: }\PY{l+s+si}{\PYZob{}}\PY{n}{hclust\PYZus{}silhouette}\PY{l+s+si}{\PYZcb{}}
\PY{l+s+s1}{KMeans clustering silhouette score: }\PY{l+s+si}{\PYZob{}}\PY{n}{kclust\PYZus{}silhouette}\PY{l+s+si}{\PYZcb{}}
\PY{l+s+s1}{GMM clustering silhouette score: }\PY{l+s+si}{\PYZob{}}\PY{n}{gmm\PYZus{}silhouette}\PY{l+s+si}{\PYZcb{}}
\PY{l+s+s1}{\PYZsq{}\PYZsq{}\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]

Hierarchical clustering vs KMeans clustering ARI: 0.4500737924478871

Hierarchical clustering vs true labels ARI: 0.0019950842701675522
KMeans clustering vs true labels ARI: 0.0022420916160579103
GMM clustering vs true labels ARI: 0.0022420916160579103

Hierarchical clustering silhouette score: 0.16310140621490865
KMeans clustering silhouette score: 0.22201717965090853
GMM clustering silhouette score: 0.1694012135651898

    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{180}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{pd}\PY{o}{.}\PY{n}{set\PYZus{}option}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{display.max\PYZus{}columns}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{k+kc}{None}\PY{p}{)}
\PY{n}{X\PYZus{}sample}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{hcluster}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{180}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
          key\_0  key\_1  key\_2  key\_3  key\_4  key\_5  key\_6  key\_7  key\_8  \textbackslash{}
hcluster
0          4098   4098   4098   4098   4098   4098   4098   4098   4098
1          4902   4902   4902   4902   4902   4902   4902   4902   4902
2          4188   4188   4188   4188   4188   4188   4188   4188   4188
3          1190   1190   1190   1190   1190   1190   1190   1190   1190
4          1983   1983   1983   1983   1983   1983   1983   1983   1983
5          1515   1515   1515   1515   1515   1515   1515   1515   1515
6          1497   1497   1497   1497   1497   1497   1497   1497   1497
7           334    334    334    334    334    334    334    334    334
8           614    614    614    614    614    614    614    614    614
9           167    167    167    167    167    167    167    167    167

          key\_9  key\_10  key\_11  mode\_0  mode\_1  time\_signature\_0  \textbackslash{}
hcluster
0          4098    4098    4098    4098    4098              4098
1          4902    4902    4902    4902    4902              4902
2          4188    4188    4188    4188    4188              4188
3          1190    1190    1190    1190    1190              1190
4          1983    1983    1983    1983    1983              1983
5          1515    1515    1515    1515    1515              1515
6          1497    1497    1497    1497    1497              1497
7           334     334     334     334     334               334
8           614     614     614     614     614               614
9           167     167     167     167     167               167

          time\_signature\_1  time\_signature\_3  time\_signature\_4  \textbackslash{}
hcluster
0                     4098              4098              4098
1                     4902              4902              4902
2                     4188              4188              4188
3                     1190              1190              1190
4                     1983              1983              1983
5                     1515              1515              1515
6                     1497              1497              1497
7                      334               334               334
8                      614               614               614
9                      167               167               167

          time\_signature\_5  popularity  duration\_ms  explicit  danceability  \textbackslash{}
hcluster
0                     4098        4098         4098      4098          4098
1                     4902        4902         4902      4902          4902
2                     4188        4188         4188      4188          4188
3                     1190        1190         1190      1190          1190
4                     1983        1983         1983      1983          1983
5                     1515        1515         1515      1515          1515
6                     1497        1497         1497      1497          1497
7                      334         334          334       334           334
8                      614         614          614       614           614
9                      167         167          167       167           167

          energy   key  loudness  mode  speechiness  acousticness  \textbackslash{}
hcluster
0           4098  4098      4098  4098         4098          4098
1           4902  4902      4902  4902         4902          4902
2           4188  4188      4188  4188         4188          4188
3           1190  1190      1190  1190         1190          1190
4           1983  1983      1983  1983         1983          1983
5           1515  1515      1515  1515         1515          1515
6           1497  1497      1497  1497         1497          1497
7            334   334       334   334          334           334
8            614   614       614   614          614           614
9            167   167       167   167          167           167

          instrumentalness  liveness  valence  tempo  time\_signature  kcluster
hcluster
0                     4098      4098     4098   4098            4098      4098
1                     4902      4902     4902   4902            4902      4902
2                     4188      4188     4188   4188            4188      4188
3                     1190      1190     1190   1190            1190      1190
4                     1983      1983     1983   1983            1983      1983
5                     1515      1515     1515   1515            1515      1515
6                     1497      1497     1497   1497            1497      1497
7                      334       334      334    334             334       334
8                      614       614      614    614             614       614
9                      167       167      167    167             167       167
\end{Verbatim}
\end{tcolorbox}
        
    Based on our investigations, the K-means and Agglomerative clustering
did ok, with silhouette scores for both methods being
\textasciitilde0.5. However, like before, GMM did terribly. However, the
adjusted rand index when compared with track genre was negligible,
meaning that the clusters formed do not at all correspond to genre, so
clustering will probably not help us with our classification problem.

    \subsection{Neural Networks}\label{neural-networks}

    We then applied neural networks to see if they could approximate the
genre as a function of the other features.

    We first put one-hot-encoded the data, scaled it, and put it onto the
GPU (if available). We used a batch size of 64 for data loading.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{185}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{device} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{device}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cuda}\PY{l+s+s1}{\PYZsq{}} \PY{k}{if} \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{is\PYZus{}available}\PY{p}{(}\PY{p}{)} \PY{k}{else} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cpu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
/home/ketexon/programming/csm148-spotiflies/.venv/lib/python3.12/site-
packages/torch/cuda/\_\_init\_\_.py:716: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/ketexon/programming/csm148-spotiflies/.venv/lib/python3.12/site-
packages/torch/cuda/\_\_init\_\_.py:905: UserWarning: CUDA initialization:
Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions
before calling NumCudaDevices() that might have already set an error? Error 803:
system has unsupported display driver / cuda driver combination (Triggered
internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  r = torch.\_C.\_cuda\_getDeviceCount() if nvml\_count < 0 else nvml\_count
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{186}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{response\PYZus{}variable} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{track\PYZus{}genre}\PY{l+s+s2}{\PYZdq{}}

\PY{c+c1}{\PYZsh{} remove string columns and one\PYZhy{}hot encode}
\PY{n}{Y\PYZus{}train} \PY{o}{=} \PY{n}{grouped\PYZus{}spotify\PYZus{}train}\PY{p}{[}\PY{n}{response\PYZus{}variable}\PY{p}{]}
\PY{n}{Y\PYZus{}val} \PY{o}{=} \PY{n}{grouped\PYZus{}spotify\PYZus{}val}\PY{p}{[}\PY{n}{response\PYZus{}variable}\PY{p}{]}

\PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{ohe\PYZus{}grouped\PYZus{}column\PYZus{}transformer\PYZus{}wo\PYZus{}genre}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{grouped\PYZus{}spotify\PYZus{}train}\PY{p}{)}
\PY{n}{X\PYZus{}val} \PY{o}{=} \PY{n}{ohe\PYZus{}grouped\PYZus{}column\PYZus{}transformer\PYZus{}wo\PYZus{}genre}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{grouped\PYZus{}spotify\PYZus{}val}\PY{p}{)}

\PY{n}{label\PYZus{}encoder} \PY{o}{=} \PY{n}{LabelEncoder}\PY{p}{(}\PY{p}{)}
\PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{grouped\PYZus{}spotify}\PY{p}{[}\PY{n}{response\PYZus{}variable}\PY{p}{]}\PY{p}{)}
\PY{n}{Y\PYZus{}train} \PY{o}{=} \PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{Y\PYZus{}train}\PY{p}{)}
\PY{n}{Y\PYZus{}val} \PY{o}{=} \PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{Y\PYZus{}val}\PY{p}{)}

\PY{c+c1}{\PYZsh{} create class weights}
\PY{n}{Y\PYZus{}train\PYZus{}unique} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{Y\PYZus{}train}\PY{p}{,} \PY{n}{return\PYZus{}counts}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{class\PYZus{}weights} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{Y\PYZus{}train}\PY{p}{)} \PY{o}{/} \PY{p}{(}\PY{n}{Y\PYZus{}train\PYZus{}unique}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n+nb}{len}\PY{p}{(}\PY{n}{Y\PYZus{}train\PYZus{}unique}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{w} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{class\PYZus{}weights}\PY{p}{)}\PY{p}{:}
    \PY{n}{label\PYZus{}encoded} \PY{o}{=} \PY{n}{Y\PYZus{}train\PYZus{}unique}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}
    \PY{n}{label\PYZus{}decoded} \PY{o}{=} \PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{inverse\PYZus{}transform}\PY{p}{(}\PY{p}{[}\PY{n}{label\PYZus{}encoded}\PY{p}{]}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
    \PY{n}{w\PYZus{}str} \PY{o}{=} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{w}\PY{l+s+si}{:}\PY{l+s+s2}{.3f}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}
    \PY{n}{count\PYZus{}str} \PY{o}{=} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{Y\PYZus{}train\PYZus{}unique}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{l+s+si}{:}\PY{l+s+s2}{,}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{label\PYZus{}decoded}\PY{+w}{ }\PY{o}{+}\PY{+w}{ }\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{:}\PY{l+s+s2}{ \PYZlt{}16}\PY{l+s+si}{\PYZcb{}}\PY{l+s+si}{\PYZob{}}\PY{n}{w\PYZus{}str}\PY{l+s+si}{:}\PY{l+s+s2}{ \PYZlt{}16}\PY{l+s+si}{\PYZcb{}}\PY{l+s+si}{\PYZob{}}\PY{n}{count\PYZus{}str}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} scale}
\PY{n}{scaler} \PY{o}{=} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}
\PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{scaler}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
\PY{n}{X\PYZus{}val} \PY{o}{=} \PY{n}{scaler}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}

\PY{c+c1}{\PYZsh{} convert to tensors}
\PY{n}{X\PYZus{}train\PYZus{}tensor} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\PY{n}{X\PYZus{}val\PYZus{}tensor} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\PY{n}{Y\PYZus{}train\PYZus{}tensor} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{Y\PYZus{}train}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{long}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\PY{n}{Y\PYZus{}val\PYZus{}tensor} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{Y\PYZus{}val}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{long}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}

\PY{n}{class\PYZus{}weights\PYZus{}tensor} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{class\PYZus{}weights}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}


\PY{c+c1}{\PYZsh{} create dataset}
\PY{n}{train\PYZus{}dataset} \PY{o}{=} \PY{n}{TensorDataset}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}tensor}\PY{p}{,} \PY{n}{Y\PYZus{}train\PYZus{}tensor}\PY{p}{)}
\PY{n}{validation\PYZus{}dataset} \PY{o}{=} \PY{n}{TensorDataset}\PY{p}{(}\PY{n}{X\PYZus{}val\PYZus{}tensor}\PY{p}{,} \PY{n}{Y\PYZus{}val\PYZus{}tensor}\PY{p}{)}

\PY{c+c1}{\PYZsh{} create dataloaders}
\PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{64}
\PY{n}{num\PYZus{}workers} \PY{o}{=} \PY{l+m+mi}{12} \PY{k}{if} \PY{n}{is\PYZus{}linux} \PY{k}{else} \PY{l+m+mi}{0}
\PY{n}{train\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}
    \PY{n}{train\PYZus{}dataset}\PY{p}{,}
    \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,}
    \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
    \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{n}{num\PYZus{}workers}\PY{p}{,}
\PY{p}{)}
\PY{n}{validation\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}
    \PY{n}{validation\PYZus{}dataset}\PY{p}{,}
    \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,}
    \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,}
    \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{n}{num\PYZus{}workers}\PY{p}{,}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
classical:      3.820           1,788
electronic:     0.569           12,002
folk:           1.895           3,604
hip-hop:        3.744           1,824
jazz:           2.854           2,393
metal:          1.876           3,640
misc:           0.380           17,962
pop:            1.268           5,386
rock:           1.040           6,569
world:          0.520           13,128
    \end{Verbatim}

    Then we defined our model. Via trial and error, we found that using ReLU
caused slow training (since the gradient is 0 if the value is negative),
and including dropout added to much variation to the training loss. In
addition, switching from softmax to log softmax helped the early
stopping algorithm from stopping too early. Thus, we kept a simple
model, with 2 fully connected hidden layers, with sigmoid activation for
the hidden layers and log softmax for the output.

We used the Adam optimizer with weigth decay for regularization using
cross entropy loss. We also used a learning rate scheduler which reduced
learning rate by 90\% every 10 epochs.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{187}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Create model}
\PY{k}{class} \PY{n+nc}{SpotifyModel}\PY{p}{(}\PY{n}{pl}\PY{o}{.}\PY{n}{LightningModule}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}
        \PY{n+nb+bp}{self}\PY{p}{,}
        \PY{n}{input\PYZus{}dim}\PY{p}{:} \PY{n+nb}{int}\PY{p}{,}
        \PY{n}{output\PYZus{}dim}\PY{p}{:} \PY{n+nb}{int}\PY{p}{,}
        \PY{n}{lr}\PY{p}{:} \PY{n+nb}{float} \PY{o}{=} \PY{l+m+mf}{0.01}\PY{p}{,}
        \PY{n}{class\PYZus{}weights}\PY{p}{:} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor} \PY{o}{=} \PY{k+kc}{None}\PY{p}{,}
        \PY{n}{hidden\PYZus{}dim1}\PY{p}{:} \PY{n+nb}{int} \PY{o}{=} \PY{l+m+mi}{64}\PY{p}{,}
        \PY{n}{hidden\PYZus{}dim2}\PY{p}{:} \PY{n+nb}{int} \PY{o}{=} \PY{l+m+mi}{64}\PY{p}{,}
        \PY{n}{weight\PYZus{}decay}\PY{p}{:} \PY{n+nb}{float} \PY{o}{=} \PY{l+m+mf}{0.01}\PY{p}{,}
    \PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{lr} \PY{o}{=} \PY{n}{lr}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{model} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{input\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim1}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Sigmoid}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim1}\PY{p}{,} \PY{n}{hidden\PYZus{}dim2}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Sigmoid}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim2}\PY{p}{,} \PY{n}{output\PYZus{}dim}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{LogSoftmax}\PY{p}{(}\PY{n}{dim}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
        \PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}classes} \PY{o}{=} \PY{n}{output\PYZus{}dim}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weight\PYZus{}decay} \PY{o}{=} \PY{n}{weight\PYZus{}decay}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{epoch\PYZus{}metrics} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{train\PYZus{}metrics\PYZus{}stack} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{validation\PYZus{}metrics\PYZus{}stack} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{loss} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{CrossEntropyLoss}\PY{p}{(}\PY{n}{weight}\PY{o}{=}\PY{n}{class\PYZus{}weights}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{model}\PY{p}{(}\PY{n}{x}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{training\PYZus{}step}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{batch}\PY{p}{,} \PY{n}{batch\PYZus{}idx}\PY{p}{)}\PY{p}{:}
        \PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{batch}
        \PY{n}{y\PYZus{}hat} \PY{o}{=} \PY{n+nb+bp}{self}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{n}{loss} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{loss}\PY{p}{(}\PY{n}{y\PYZus{}hat}\PY{p}{,} \PY{n}{y}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{train\PYZus{}loss}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{loss}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{train\PYZus{}metrics\PYZus{}stack}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{\PYZob{}}
            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{loss}\PY{p}{,}
        \PY{p}{\PYZcb{}}\PY{p}{)}
        \PY{k}{return} \PY{n}{loss}

    \PY{k}{def} \PY{n+nf}{validation\PYZus{}step}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{batch}\PY{p}{,} \PY{n}{batch\PYZus{}idx}\PY{p}{)}\PY{p}{:}
        \PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{batch}
        \PY{n}{y\PYZus{}hat} \PY{o}{=} \PY{n+nb+bp}{self}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{n}{loss} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{loss}\PY{p}{(}\PY{n}{y\PYZus{}hat}\PY{p}{,} \PY{n}{y}\PY{p}{)}
        \PY{n}{preds} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{y\PYZus{}hat}\PY{p}{,} \PY{n}{dim}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{acc} \PY{o}{=} \PY{p}{(}\PY{n}{preds} \PY{o}{==} \PY{n}{y}\PY{p}{)}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{validation\PYZus{}metrics\PYZus{}stack}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{\PYZob{}}
            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{loss}\PY{p}{,}
            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{acc}
        \PY{p}{\PYZcb{}}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{val\PYZus{}loss}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{loss}\PY{p}{)}
        \PY{k}{return} \PY{n}{loss}

    \PY{k}{def} \PY{n+nf}{on\PYZus{}validation\PYZus{}epoch\PYZus{}end}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} sum in validation stack}
        \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{validation\PYZus{}metrics\PYZus{}stack}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{k}{return}
        \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{train\PYZus{}metrics\PYZus{}stack}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{k}{return}
        \PY{n}{val\PYZus{}loss} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{stack}\PY{p}{(}\PY{p}{[}\PY{n}{x}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{validation\PYZus{}metrics\PYZus{}stack}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
        \PY{n}{val\PYZus{}acc} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{stack}\PY{p}{(}\PY{p}{[}\PY{n}{x}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{validation\PYZus{}metrics\PYZus{}stack}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
        \PY{n}{train\PYZus{}loss} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{stack}\PY{p}{(}\PY{p}{[}\PY{n}{x}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{train\PYZus{}metrics\PYZus{}stack}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{epoch\PYZus{}metrics}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{\PYZob{}}
            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{val\PYZus{}loss}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{val\PYZus{}acc}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{train\PYZus{}loss}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{p}{,}
        \PY{p}{\PYZcb{}}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} clear stack}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{validation\PYZus{}metrics\PYZus{}stack} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{train\PYZus{}metrics\PYZus{}stack} \PY{o}{=} \PY{p}{[}\PY{p}{]}


    \PY{k}{def} \PY{n+nf}{configure\PYZus{}optimizers}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{n}{optimizer} \PY{o}{=} \PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{lr}\PY{p}{,} \PY{n}{weight\PYZus{}decay}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weight\PYZus{}decay}\PY{p}{)}
        \PY{n}{scheduler} \PY{o}{=} \PY{n}{optim}\PY{o}{.}\PY{n}{lr\PYZus{}scheduler}\PY{o}{.}\PY{n}{StepLR}\PY{p}{(}\PY{n}{optimizer}\PY{p}{,} \PY{n}{step\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}
        \PY{k}{return} \PY{p}{[}\PY{n}{optimizer}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{scheduler}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    To pick an initial learning rate, we used PyTorch's \texttt{lr\_find}
function, which gave us an initial learning rate of about 0.002. We then
trained the model, which stopped early at 27 epochs.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{input\PYZus{}dim} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
\PY{n}{output\PYZus{}dim} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}
    \PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{classes\PYZus{}}
\PY{p}{)}
\PY{n}{lr} \PY{o}{=} \PY{l+m+mf}{0.001}

\PY{n}{model} \PY{o}{=} \PY{n}{SpotifyModel}\PY{p}{(}
    \PY{n}{input\PYZus{}dim}\PY{p}{,}
    \PY{n}{output\PYZus{}dim}\PY{p}{,}
    \PY{n}{lr}\PY{o}{=}\PY{n}{lr}\PY{p}{,}
    \PY{n}{class\PYZus{}weights}\PY{o}{=}\PY{n}{class\PYZus{}weights\PYZus{}tensor}
\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}

\PY{c+c1}{\PYZsh{} train model}
\PY{n}{early\PYZus{}stopping} \PY{o}{=} \PY{n}{pl\PYZus{}callbacks}\PY{o}{.}\PY{n}{EarlyStopping}\PY{p}{(}
    \PY{n}{monitor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{n}{patience}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,}
    \PY{n}{min\PYZus{}delta}\PY{o}{=}\PY{l+m+mf}{0.0001}\PY{p}{,}
    \PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min}\PY{l+s+s1}{\PYZsq{}}
\PY{p}{)}

\PY{n}{use\PYZus{}checkpoint} \PY{o}{=} \PY{k+kc}{False}
\PY{n}{ckpt\PYZus{}path}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{epoch=12\PYZhy{}step=18499.ckpt}\PY{l+s+s2}{\PYZdq{}} \PY{k}{if} \PY{n}{use\PYZus{}checkpoint} \PY{k}{else} \PY{k+kc}{None}
\PY{n}{trainer} \PY{o}{=} \PY{n}{pl}\PY{o}{.}\PY{n}{Trainer}\PY{p}{(}
    \PY{n}{max\PYZus{}epochs}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,}
    \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}
        \PY{n}{early\PYZus{}stopping}
    \PY{p}{]}
\PY{p}{)}

\PY{c+c1}{\PYZsh{} tune the model}
\PY{k+kn}{from} \PY{n+nn}{lightning}\PY{n+nn}{.}\PY{n+nn}{pytorch}\PY{n+nn}{.}\PY{n+nn}{tuner}\PY{n+nn}{.}\PY{n+nn}{tuning} \PY{k+kn}{import} \PY{n}{Tuner}
\PY{n}{tuner} \PY{o}{=} \PY{n}{Tuner}\PY{p}{(}\PY{n}{trainer}\PY{p}{)}

\PY{n}{lr\PYZus{}finder} \PY{o}{=} \PY{n}{tuner}\PY{o}{.}\PY{n}{lr\PYZus{}find}\PY{p}{(}
    \PY{n}{model}\PY{p}{,}
    \PY{n}{train\PYZus{}dataloaders}\PY{o}{=}\PY{n}{train\PYZus{}loader}\PY{p}{,}
    \PY{n}{val\PYZus{}dataloaders}\PY{o}{=}\PY{n}{validation\PYZus{}loader}\PY{p}{,}
    \PY{n}{min\PYZus{}lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}10}\PY{p}{,}
    \PY{n}{max\PYZus{}lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}1}\PY{p}{,}
    \PY{n}{num\PYZus{}training}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,}
\PY{p}{)}

\PY{n}{model}\PY{o}{.}\PY{n}{lr} \PY{o}{=} \PY{n}{lr\PYZus{}finder}\PY{o}{.}\PY{n}{suggestion}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Learning rate: }\PY{l+s+si}{\PYZob{}}\PY{n}{model}\PY{o}{.}\PY{n}{lr}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{trainer}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
    \PY{n}{model}\PY{p}{,}
    \PY{n}{train\PYZus{}loader}\PY{p}{,}
    \PY{n}{validation\PYZus{}loader}\PY{p}{,}
    \PY{n}{ckpt\PYZus{}path}\PY{o}{=}\PY{n}{ckpt\PYZus{}path}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
/home/ketexon/programming/csm148-spotiflies/.venv/lib/python3.12/site-
packages/torch/cuda/\_\_init\_\_.py:716: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Finding best initial lr:  96\%|█████████▌| 96/100 [00:00<00:00,
138.32it/s]`Trainer.fit` stopped: `max\_steps=100` reached.
Finding best initial lr: 100\%|██████████| 100/100 [00:00<00:00, 132.55it/s]
Learning rate set to 0.0019498445997580478
Restoring states from the checkpoint path at /home/ketexon/programming/csm148-
spotiflies/.lr\_find\_71f92498-ffbf-4528-aab4-a052b2bdc14c.ckpt
Restored all states from the checkpoint at /home/ketexon/programming/csm148-
spotiflies/.lr\_find\_71f92498-ffbf-4528-aab4-a052b2bdc14c.ckpt

  | Name  | Type             | Params | Mode
---------------------------------------------------
0 | model | Sequential       | 6.7 K  | train
1 | loss  | CrossEntropyLoss | 0      | train
---------------------------------------------------
6.7 K     Trainable params
0         Non-trainable params
6.7 K     Total params
0.027     Total estimated model params size (MB)
8         Modules in train mode
0         Modules in eval mode
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Learning rate: 0.0019498445997580478
Epoch 27: 100\%|██████████| 1423/1423 [00:14<00:00, 100.88it/s, v\_num=55]
    \end{Verbatim}

    We then plotted a classification report on the model.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} evaluate model}
\PY{n}{model}\PY{o}{.}\PY{n}{eval}\PY{p}{(}\PY{p}{)}
\PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{outputs} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}\PY{p}{(}\PY{n}{X\PYZus{}val\PYZus{}tensor}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}
    \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{predicted} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{outputs}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{value\PYZus{}counts} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{predicted}\PY{p}{)}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}
    \PY{k}{for} \PY{n}{label} \PY{o+ow}{in} \PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{classes\PYZus{}}\PY{p}{:}
        \PY{n}{class\PYZus{}encoded} \PY{o}{=} \PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{p}{[}\PY{n}{label}\PY{p}{]}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{count} \PY{o}{=} \PY{n}{value\PYZus{}counts}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{n}{class\PYZus{}encoded}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{label}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{: }\PY{l+s+si}{\PYZob{}}\PY{n}{count}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{class\PYZus{}report} \PY{o}{=} \PY{n}{classification\PYZus{}report}\PY{p}{(}
        \PY{n}{Y\PYZus{}val}\PY{p}{,}
        \PY{n}{predicted}\PY{p}{,}
        \PY{n}{labels}\PY{o}{=}\PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{classes\PYZus{}}\PY{p}{)}\PY{p}{)}\PY{p}{,}
        \PY{n}{target\PYZus{}names}\PY{o}{=}\PY{n}{label\PYZus{}encoder}\PY{o}{.}\PY{n}{classes\PYZus{}}\PY{p}{,}
        \PY{n}{zero\PYZus{}division}\PY{o}{=}\PY{l+m+mi}{1}
    \PY{p}{)}
\PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{class\PYZus{}report}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
classical: 4629
electronic: 4627
folk: 2029
hip-hop: 0
jazz: 890
metal: 6415
misc: 3462
pop: 0
rock: 609
world: 105
              precision    recall  f1-score   support

   classical       0.10      0.77      0.18       607
  electronic       0.26      0.30      0.28      4026
        folk       0.09      0.15      0.11      1218
     hip-hop       1.00      0.00      0.00       578
        jazz       0.07      0.07      0.07       830
       metal       0.17      0.90      0.28      1203
        misc       0.25      0.15      0.19      5931
         pop       1.00      0.00      0.00      1825
        rock       0.09      0.03      0.04      2227
       world       0.30      0.01      0.01      4321

    accuracy                           0.17     22766
   macro avg       0.33      0.24      0.12     22766
weighted avg       0.30      0.17      0.13     22766

    \end{Verbatim}

    The classification shows pretty bad results, with overall 17\% accuracy
(which is hardly better than uniform, at 10\%).

However, something interesting of note is that the recall for classical
and metal was significantly high. This means that if the model predicted
that a song was metal or classical, it was likely correct. Something
else of note was that pop and hip hop, the most and least represented
classes, had 0 predictions. This was stranger, but through othe models,
we saw that the class imbalance disproportionately made more represented
classes be predicted. When we added class weights, this might have
overcompensated for pop, leading to pop having poor predictions, and
undercompensated for hip-hop.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Graph epoch metrics}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}

\PY{c+c1}{\PYZsh{} plot epoch to val loss and train loss}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{x}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{model}\PY{o}{.}\PY{n}{epoch\PYZus{}metrics}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{x}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{model}\PY{o}{.}\PY{n}{epoch\PYZus{}metrics}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_142_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} plot accuracy per epoch}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{x}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{model}\PY{o}{.}\PY{n}{epoch\PYZus{}metrics}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_code_files/project_code_143_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Plotting the validation and training loss, as well as the model accuracy
over epochs, we can see that thte loss did shrink over time, with
accuracy never really increasing. This could be an indicator that cross
entropy was not a good loss function.


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
